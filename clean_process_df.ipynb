{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45648af-07aa-457f-b47b-6d28ce514a95",
   "metadata": {},
   "source": [
    "# Clean and process the dataframe from survey results with:\n",
    "\n",
    "* Merge all human survey dataframes into 1\n",
    "* Calculate soft labels and predictions for humans\n",
    "* Add human entropy and t2c\n",
    "* Add model and explainer soft labels to df\n",
    "* Add alignment of all 3 pairs to df\n",
    "* Add top and bottom confidence values for all three agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d24f054-1034-472b-947e-78cacd6efa5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "# import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from scipy import stats\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "from utils.utils import read_lists, load_image, ensure_dir\n",
    "from utils.df_utils import convert_string_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45bae1c-1b76-4cce-b53f-a9ebab01e1fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.path.join('saved', 'ADE20K', 'survey_results', 'ADE20K_soft_labels')\n",
    "MEASUREMENT_COLUMN_NAMES = ['selectedAttrs', 'attrUncs']\n",
    "TASK_METADATA_COLUMN_NAMES = ['filename', 'task', 'concept_group']\n",
    "EXPLAINER_DIRNAME = 'saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/KD_baseline_explainer/hparam_search/0523_164052/best'\n",
    "\n",
    "CONGRUENCY_PATH_TEMPLATE = os.path.join(EXPLAINER_DIRNAME, '{}_paths.txt')\n",
    "CONGRUENT_PATHS_PATH = CONGRUENCY_PATH_TEMPLATE.format('congruent')\n",
    "INCONGRUENT_PATHS_PATH = CONGRUENCY_PATH_TEMPLATE.format('incongruent')\n",
    "\n",
    "MODEL_DIRNAME = 'saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga'\n",
    "# Path to where images in ADE20K are stored. (Prefix to path in congruent/incongruent paths files)\n",
    "ADE20K_PARENT_DIR = os.path.join('data', 'broden1_224', 'images')\n",
    "CSV_SAVE_PATH = os.path.join(os.path.dirname(RESULTS_DIR), 'processed_results_{}_samples.csv')\n",
    "\n",
    "SCENE_CATEGORIES_PATH = os.path.join('data', 'ade20k', 'scene_categories.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dfcc8bb-b1f2-416e-864c-113e244e5875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 2-way dictionary mapping from category <-> index\n",
    "scene_categories = read_lists(SCENE_CATEGORIES_PATH)\n",
    "scene_categories_dict = {}\n",
    "for idx, category in enumerate(scene_categories):\n",
    "    scene_categories_dict[idx] = category\n",
    "    scene_categories_dict[category] = idx\n",
    "n_categories = len(scene_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb8572-ba44-4533-b5ab-fff600c3e8e6",
   "metadata": {},
   "source": [
    "### Merge all human survey dataframes into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b7573f-523d-48f7-a6af-f6588c49bc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 000_uncertainty-annotation_PARTICIPANT_SESSION_2023-05-03_17h26.47.486.csv\n",
      "Processing 001_uncertainty-annotation_PARTICIPANT_SESSION_2023-05-03_17h35.07.423.csv\n",
      "Processing 002_uncertainty-annotation_PARTICIPANT_SESSION_2023-05-03_17h40.48.508.csv\n",
      "Processing 003_uncertainty-annotation_PARTICIPANT_SESSION_2023-05-03_17h48.15.198.csv\n",
      "Processing 004_uncertainty-annotation_PARTICIPANT_SESSION_2023-05-03_17h55.01.103.csv\n",
      "Processing 005_uncertainty-annotation_PARTICIPANT_SESSION_2023-05-03_18h00.12.78.csv\n",
      "Processing 006_uncertainty-annotation_PARTICIPANT_SESSION_2023-05-03_18h05.33.934.csv\n",
      "Processing 007_uncertainty-annotation_PARTICIPANT_SESSION_2023-05-03_18h11.11.320.csv\n",
      "Processing 008_uncertainty-annotation_PARTICIPANT_SESSION_2023-05-04_09h23.31.131.csv\n",
      "Processing 009_uncertainty-annotation_PARTICIPANT_SESSION_2023-05-04_09h28.52.510.csv\n",
      "Total of 300 samples\n"
     ]
    }
   ],
   "source": [
    "csv_paths = []\n",
    "\n",
    "for filename in os.listdir(RESULTS_DIR):\n",
    "    if filename.endswith('csv'):\n",
    "        csv_paths.append(os.path.join(RESULTS_DIR, filename))\n",
    "\n",
    "csv_paths = sorted(csv_paths)\n",
    "\n",
    "df_list = []\n",
    "for csv_path in csv_paths:\n",
    "    print(\"Processing {}\".format(os.path.basename(csv_path))) \n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Separate dataframe into rows with measurements and with metadata\n",
    "    measurement_df = df[MEASUREMENT_COLUMN_NAMES]\n",
    "    metadata_df = df.drop(MEASUREMENT_COLUMN_NAMES, axis=1)\n",
    "\n",
    "    # Drop empty rows\n",
    "    measurement_df = measurement_df.dropna()\n",
    "    # Drop rows without data in task metadata columns\n",
    "    metadata_df = metadata_df.dropna(subset=TASK_METADATA_COLUMN_NAMES)\n",
    "\n",
    "    # Remove columns that are empty\n",
    "    metadata_df = metadata_df.dropna(axis=1)\n",
    "    \n",
    "    # congruents = []\n",
    "    # for filename in metadata_df['filename']:\n",
    "    #     path = os.path.join(ADE20K_PARENT_DIR, filename)\n",
    "    #     if path in congruent_paths:\n",
    "    #         congruents.append(1)\n",
    "    #     elif path in incongruent_paths:\n",
    "    #         congruents.append(0)\n",
    "    #     else:\n",
    "    #         raise ValueError(\"Path {} not found in congruent or incongruent paths... :0\".format(path))\n",
    "    # metadata_df['congruent'] = congruents\n",
    "\n",
    "    # Assert that the two DFs have the same number of rows\n",
    "    assert len(metadata_df) == len(measurement_df), \"Uneven length data frames. Metadata length: {} Measurement length: {}\".format(\n",
    "        len(metadata_df), len(measurement_df))\n",
    "\n",
    "    # Reset indices to allow for joining appropriately\n",
    "    metadata_df = metadata_df.reset_index(drop=True)\n",
    "    measurement_df = measurement_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Join the data frames\n",
    "    df = pd.concat([metadata_df, measurement_df], axis=1)\n",
    "    assert len(df) == len(metadata_df)\n",
    "\n",
    "    # Add dataframe to list of dataframes\n",
    "    df_list.append(df)\n",
    "    \n",
    "# Concatenate rows of all dataframes together\n",
    "df = pd.concat(df_list)\n",
    "n_samples = len(df)\n",
    "print(\"Total of {} samples\".format(n_samples))\n",
    "# n_congruent = len(df[df['congruent'] == 1])\n",
    "# n_incongruent = len(df[df['congruent'] == 0])\n",
    "# print(\"Total of {} congruent samples and {} incongruent samples\".format(n_congruent, n_incongruent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7155ba-7e3b-4aa5-b58a-4d375af45bbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate soft labels and predictions for humans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a066e577-70da-4d03-a9d6-254c4a35024c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating human soft labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 28880.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated human outputs, probabilities, and predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating human soft labels\")\n",
    "human_probabilities = []\n",
    "human_outputs = []\n",
    "human_predictions = []\n",
    "for row in tqdm(df['attrUncs']):\n",
    "    soft_label = np.zeros(n_categories)\n",
    "    # Each 'score' item is a dictionary of class and certainty amount\n",
    "    row = json.loads(row)\n",
    "    for item in row:\n",
    "        category = item['label']\n",
    "        certainty = item['y'] / 100.0\n",
    "        category_idx = scene_categories_dict[category]\n",
    "        soft_label[category_idx] = certainty\n",
    "    label_sum = np.sum(soft_label)\n",
    "    human_outputs.append(soft_label)\n",
    "\n",
    "    # Normalize to sum to one\n",
    "    soft_label = soft_label / label_sum\n",
    "    # Assert the soft label sums to 1\n",
    "    assert np.abs(np.sum(soft_label) - 1.0) < 1e-5\n",
    "\n",
    "    human_probabilities.append(soft_label)\n",
    "    human_predictions.append(np.argmax(soft_label))\n",
    "\n",
    "df['human_probabilities'] = human_probabilities\n",
    "df['human_outputs'] = human_outputs\n",
    "df['human_predictions'] = human_predictions\n",
    "print(\"Calculated human outputs, probabilities, and predictions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d7fcd-4f1d-477c-8496-7b8604fae631",
   "metadata": {},
   "source": [
    "### Add human entropy and t2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f8c1b1-5557-4628-86aa-8243b144d084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_2_confusion(soft_labels):\n",
    "    '''\n",
    "    Given soft label distribution, calculate difference between top 2 labels\n",
    "\n",
    "    Arg(s):\n",
    "        soft_labels : N x C np.array\n",
    "            soft label array for N samples and C class predictions\n",
    "\n",
    "    Returns:\n",
    "        confusion : N-dim np.array\n",
    "            confusion for each sample\n",
    "    '''\n",
    "    # Sort soft labels ascending\n",
    "    sorted_soft_labels = np.sort(soft_labels, axis=-1)\n",
    "    # Calculate difference of p(x) for top 2 classes\n",
    "    top_2_difference = sorted_soft_labels[:, -1] - sorted_soft_labels[:, -2]\n",
    "    # Confusion = 1 - difference (higher is worse)\n",
    "    top_2_confusion = 1 - top_2_difference\n",
    "\n",
    "    return top_2_confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8571f340-67d2-4c1e-ba86-32b5119915d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated entropy, t2c for human labels\n"
     ]
    }
   ],
   "source": [
    "# Calculate entropy\n",
    "entropy = stats.entropy(human_probabilities, axis=1)\n",
    "df['human_entropy'] = entropy\n",
    "\n",
    "# Top 2 confusion\n",
    "human_t2c = get_top_2_confusion(human_outputs)\n",
    "df['human_t2c'] = human_t2c\n",
    "\n",
    "print(\"Calculated entropy, t2c for human labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0d1da-0de3-4791-b004-f12745382ac3",
   "metadata": {},
   "source": [
    "### Add model and explainer soft labels to df, add KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd59c9ce-d0f0-4cb8-ae45-8c9fa8971dde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added model and explainer's outputs, probabilities, and predictions to dataframe\n",
      "Added model and explainer entropy, t2c to dataframe\n"
     ]
    }
   ],
   "source": [
    "# Obtain explainer softmax probabilities\n",
    "explainer_outputs_path = os.path.join(EXPLAINER_DIRNAME, 'val_outputs_predictions.pth')\n",
    "explainer_out = torch.load(explainer_outputs_path)\n",
    "explainer_probabilities = explainer_out['probabilities']\n",
    "\n",
    "# Obtainer model's softmax probabilities\n",
    "model_outputs_path = os.path.join(MODEL_DIRNAME, 'val_outputs_predictions.pth')\n",
    "model_out = torch.load(model_outputs_path)\n",
    "model_probabilities = model_out['probabilities']\n",
    "\n",
    "# Obtain mapping from image name to index of validation set\n",
    "image_labels_path = 'data/ade20k/full_ade20k_imagelabels.pth'\n",
    "image_labels = torch.load(image_labels_path)\n",
    "val_images = image_labels['val']\n",
    "val_images = [path.split('images/')[-1] for path in val_images]\n",
    "val_name_idx_dict = {}\n",
    "for idx, image_name in enumerate(val_images):\n",
    "    val_name_idx_dict[image_name] = idx\n",
    "\n",
    "\n",
    "# For each row in the data frame, store the explainers' and models' outputs\n",
    "for name, outputs in zip(['explainer', 'model'], [explainer_out, model_out]):\n",
    "    for output_type in ['outputs', 'probabilities', 'predictions']:\n",
    "        cur_outputs = outputs[output_type]\n",
    "        accumulator = []\n",
    "        for image_name in df['filename']:\n",
    "            val_idx = val_name_idx_dict[image_name]\n",
    "            cur_item = cur_outputs[val_idx]\n",
    "            accumulator.append(cur_item)\n",
    "        df['{}_{}'.format(name, output_type)] = accumulator\n",
    "        \n",
    "    # add entropy and t2c for model\n",
    "    # if name == 'model':\n",
    "    cur_probabilities = np.stack(df['{}_probabilities'.format(name)].to_numpy(), axis=0)\n",
    "    cur_outputs = np.stack(df['{}_outputs'.format(name)].to_numpy(), axis=0)\n",
    "    cur_entropy = stats.entropy(cur_probabilities, axis=1)\n",
    "    cur_t2c = get_top_2_confusion(cur_outputs)\n",
    "    # Min-max scale t2c to be between [0, 1]\n",
    "    min_t2c = np.amin(cur_t2c)\n",
    "    max_t2c = np.amax(cur_t2c)\n",
    "    scaled_cur_t2c = (cur_t2c - min_t2c) / (max_t2c - min_t2c)\n",
    "    df['{}_entropy'.format(name)] = cur_entropy\n",
    "    df['{}_t2c'.format(name)] = cur_t2c\n",
    "    df['{}_scaled_t2c'.format(name)] = scaled_cur_t2c\n",
    "        \n",
    "print(\"Added model and explainer's outputs, probabilities, and predictions to dataframe\")\n",
    "print(\"Added model and explainer entropy, t2c to dataframe\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287fa8a6-2cbb-4f78-828d-8aefcbdedcdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added KL divergence between model and explainer probabilities\n"
     ]
    }
   ],
   "source": [
    "# KL divergence between model-explainer\n",
    "p = np.stack(df['explainer_probabilities'], axis=0)\n",
    "q = np.stack(df['model_probabilities'], axis=0)\n",
    "kl_model_explainer = stats.entropy(pk=p, qk=q, axis=1)\n",
    "df['kl_model_explainer'] = kl_model_explainer\n",
    "print(\"Added KL divergence between model and explainer probabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa2206-9191-43d0-b4fb-0ccdee36436f",
   "metadata": {},
   "source": [
    "### Add alignment and top and bottom confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb4a7225-e3be-4ec8-aafe-99ff09672ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added all three pairs of alignment to the dataframe\n",
      "Added top and bottom confidences for all agents to df\n"
     ]
    }
   ],
   "source": [
    "agent_pairs = [\n",
    "    ('human', 'explainer'),\n",
    "    ('human', 'model'),\n",
    "    ('model', 'explainer')]\n",
    "for agent1, agent2 in agent_pairs:\n",
    "    agent1_predictions = df['{}_predictions'.format(agent1)]\n",
    "    agent2_predictions = df['{}_predictions'.format(agent2)]\n",
    "    alignment = np.where(agent1_predictions == agent2_predictions, 1, 0)\n",
    "    df['{}_{}_alignment'.format(agent1, agent2)] = alignment\n",
    "print(\"Added all three pairs of alignment to the dataframe\")\n",
    "\n",
    "def add_confidence(df, \n",
    "                   agent, \n",
    "                   top=True):\n",
    "    column_name = '{}_probabilities'.format(agent)\n",
    "    assert column_name in df.columns\n",
    "    \n",
    "    # Convert str -> numpy if necessary\n",
    "    if type(df[column_name][0]) == str:\n",
    "        df = convert_string_columns(df, [column_name])\n",
    "    \n",
    "    # Calculate confidence scores and add to DF\n",
    "    probabilities = np.stack(df[column_name].to_numpy(), axis=0)\n",
    "    if top:\n",
    "        confidence = np.amax(probabilities, axis=1)\n",
    "        df['{}_top_confidence'.format(agent)] = confidence\n",
    "    else:  # confidence of bottom logit\n",
    "        confidence = np.amin(probabilities, axis=1)\n",
    "        df['{}_bottom_confidence'.format(agent)] = confidence\n",
    "    return df\n",
    "\n",
    "agents = ['human', 'model', 'explainer']\n",
    "\n",
    "for agent in agents:\n",
    "    df = add_confidence(\n",
    "        df,\n",
    "        agent,\n",
    "        top=True)\n",
    "    df = add_confidence(\n",
    "        df,\n",
    "        agent,\n",
    "        top=False)\n",
    "\n",
    "print(\"Added top and bottom confidences for all agents to df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d22d9563-d8c4-4ae8-b6f0-8c1e9432af1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved csv to saved/ADE20K/survey_results/processed_results_300_samples.csv\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(df)\n",
    "csv_save_path = CSV_SAVE_PATH.format(n_samples)\n",
    "if os.path.exists(csv_save_path):\n",
    "    print(\"File already exists at {}. Rename/remove it in order to save\".format(csv_save_path))\n",
    "else:\n",
    "    df.to_csv(csv_save_path)\n",
    "    print(\"Saved csv to {}\".format(csv_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30311df8-084b-4ded-8703-48250ba5977e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-correlation",
   "language": "python",
   "name": "model-correlation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
