{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "122f6d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "from train import main as train_fn\n",
    "from predict import predict\n",
    "from parse_config import ConfigParser\n",
    "import datasets.datasets as module_data\n",
    "from utils.utils import read_json, ensure_dir, informal_log, write_lists\n",
    "from utils.model_utils import prepare_device\n",
    "from utils.attribute_utils import partition_paths_by_congruency\n",
    "from model import metric as module_metric\n",
    "from model import loss as module_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53f78a-550d-4439-b011-1197514684fb",
   "metadata": {},
   "source": [
    "### Load hparam search variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84268030-d5ed-4bc7-8bf4-f9cf628a7e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_path = 'configs/train_ade20k_explainer_KD.json'\n",
    "debug = False\n",
    "if debug:\n",
    "    learning_rates = [1e-6] #, 1e-5, 1e-4, 1e-3, 5e-2, 1e-2, 5e-1, 1e-1]\n",
    "    weight_decays = [0, 1e-1] #, 1e-2, 1e-3]\n",
    "else:\n",
    "    learning_rates = [1e-6, 1e-5, 1e-4, 1e-3, 5e-2, 1e-2, 5e-1, 1e-1]\n",
    "    weight_decays = [0, 1e-1, 1e-2, 1e-3]\n",
    "\n",
    "config_json = read_json(config_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b42639",
   "metadata": {},
   "source": [
    "### Create train and validation datasets outside of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb3631c-3870-4c71-b884-f421484b3e2a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_args = config_json['dataset']['args']\n",
    "train_dataset = module_data.KDDataset(split='train', **dataset_args)\n",
    "val_dataset = module_data.KDDataset(split='val', **dataset_args)\n",
    "\n",
    "dataloader_args = config_json['data_loader']['args']\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    **dataloader_args)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd71c3b-7f35-4d86-854d-26a15e9f72c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best = {\n",
    "    'lr': -1,\n",
    "    'wd': -1,\n",
    "    'val_acc': -1\n",
    "}\n",
    "n_trials = len(learning_rates) * len(weight_decays)\n",
    "trial_idx = 1\n",
    "timestamp = datetime.now().strftime(r'%m%d_%H%M%S')\n",
    "\n",
    "# Logging\n",
    "log_path = os.path.join(config_json['trainer']['save_dir'], timestamp, 'log.txt')\n",
    "ensure_dir(os.path.dirname(log_path))\n",
    "informal_log(\"Hyperparameter search\", log_path)\n",
    "informal_log(\"Learning rates: {}\".format(learning_rates), log_path)\n",
    "informal_log(\"Weight decays: {}\".format(weight_decays), log_path)\n",
    "\n",
    "# Debug mode\n",
    "if debug:\n",
    "    config_json['trainer']['epochs'] = 1\n",
    "    \n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        # Update config json\n",
    "        config_json['optimizer']['args'].update({\n",
    "            'lr': lr,\n",
    "            'weight_decay': wd\n",
    "        })\n",
    "        \n",
    "        # Create run ID for trial\n",
    "        itr_timestamp = datetime.now().strftime(r'%m%d_%H%M%S')\n",
    "        informal_log(\"[{}] Trial {}/{}: LR = {} WD = {}\".format(\n",
    "            itr_timestamp, trial_idx, n_trials, lr, wd), log_path)\n",
    "        run_id = os.path.join(timestamp, 'trials', 'lr_{}-wd_{}'.format(lr, wd))\n",
    "        config = ConfigParser(config_json, run_id=run_id)\n",
    "        print(config.config['optimizer']['args'])\n",
    "        \n",
    "        # Train model\n",
    "        model = train_fn(\n",
    "            config=config, \n",
    "            train_data_loader=train_dataloader,\n",
    "            val_data_loader=val_dataloader)\n",
    "        \n",
    "        # Restore model\n",
    "        model_restore_path = os.path.join(config.save_dir, 'model_best.pth')\n",
    "        \n",
    "        model.restore_model(model_restore_path)\n",
    "        print(\"restored model\")\n",
    "        # Run on validation set using predict function\n",
    "        device, device_ids = prepare_device(config_json['n_gpu'])\n",
    "        metric_fns = [getattr(module_metric, met) for met in config_json['metrics']]\n",
    "        loss_fn = getattr(module_loss, config_json['loss'])\n",
    "        trial_path = os.path.dirname(os.path.dirname(model_restore_path))\n",
    "        output_save_path = os.path.join(trial_path, \"val_outputs.pth\")\n",
    "        log_save_path = os.path.join(trial_path, \"val_metrics.pth\")\n",
    "        \n",
    "        validation_data = predict(\n",
    "            data_loader=val_dataloader,\n",
    "            model=model,\n",
    "            metric_fns=metric_fns,\n",
    "            device=device,\n",
    "            loss_fn=loss_fn,\n",
    "            output_save_path=output_save_path,\n",
    "            log_save_path=log_save_path)\n",
    "       \n",
    "        # Obtain accuracy and compare to previous best\n",
    "        print(validation_data['metrics'].keys())\n",
    "        val_accuracy = validation_data['metrics']['accuracy']\n",
    "        if val_accuracy > best['val_acc']:\n",
    "            best.update({\n",
    "                'lr': lr,\n",
    "                'wd': wd,\n",
    "                'val_acc': val_accuracy\n",
    "            })\n",
    "            informal_log(\"Best accuracy of {:.3f} with lr={} and wd={}\".format(val_accuracy, lr, wd), log_path)\n",
    "            informal_log(\"Trial path: {}\".format(trial_path), log_path)\n",
    "            # Copy model and outputs to 1 directory for easy access\n",
    "            best_save_dir = os.path.join(os.path.dirname(os.path.dirname(trial_path)), 'best')\n",
    "            ensure_dir(best_save_dir)\n",
    "            best_outputs_save_path = os.path.join(best_save_dir, 'outputs.pth')\n",
    "            best_model_save_path = os.path.join(best_save_dir, 'model.pth')\n",
    "            torch.save(validation_data['logits'], best_outputs_save_path)\n",
    "            model.save_model(best_model_save_path)\n",
    "            informal_log(\"Saved model and outputs to {}\".format(best_save_dir), log_path)\n",
    "            \n",
    "            \n",
    "        trial_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8509c-db35-4a2f-989d-b0c36a0eeaf7",
   "metadata": {},
   "source": [
    "## Post Processing of results before survey processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04857d8b-f8a9-4dbd-99ee-3a970eb498d1",
   "metadata": {},
   "source": [
    "### From the best outputs, also obtain probabilities and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "effdd8ec-6ac0-46be-9bac-734460b8e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved outputs, probabilities, and predictions to saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/KD_baseline_explainer/hparam_search/0523_164052/best/val_outputs_predictions.pth\n"
     ]
    }
   ],
   "source": [
    "best_output_path = 'saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/KD_baseline_explainer/hparam_search/0523_164052/best/outputs.pth'\n",
    "best_output_dir = os.path.dirname(best_output_path)\n",
    "\n",
    "outputs = torch.load(best_output_path)\n",
    "softmax = torch.softmax(outputs, dim=1)\n",
    "outputs = outputs.cpu().numpy()\n",
    "softmax = softmax.cpu().numpy()\n",
    "predictions = np.argmax(softmax, axis=1)\n",
    "\n",
    "data = {\n",
    "    'outputs': outputs,\n",
    "    'probabilities': softmax,\n",
    "    'predictions': predictions\n",
    "}\n",
    "data_save_path = os.path.join(best_output_dir, 'val_outputs_predictions.pth')\n",
    "torch.save(data, data_save_path)\n",
    "print(\"Saved outputs, probabilities, and predictions to {}\".format(data_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d422d8-15ba-4596-8575-3c1c2ba264f1",
   "metadata": {},
   "source": [
    "### Obtain congruent and incongruent paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55059242-db20-498f-a6c6-d36b0bb8c44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4442it [00:00, 2446310.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2764 congruent paths and 1678 incongruent paths\n",
      "Wrote congruent paths to saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/KD_baseline_explainer/hparam_search/0523_164052/best/congruent_paths.txt and incongruent paths to saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/KD_baseline_explainer/hparam_search/0523_164052/best/incongruent_paths.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load predictions and paths to images\n",
    "explainer_predictions = predictions\n",
    "model_predictions_path = 'saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/val_outputs_predictions.pth'\n",
    "model_predictions = torch.load(model_predictions_path)['predictions']\n",
    "\n",
    "paths_path = 'data/ade20k/full_ade20k_imagelabels.pth'\n",
    "val_paths = torch.load(paths_path)['val']\n",
    "\n",
    "# Sanity checks\n",
    "assert explainer_predictions.shape == model_predictions.shape\n",
    "assert len(explainer_predictions) == len(val_paths)\n",
    "\n",
    "# Obtain congruent and incongruent paths\n",
    "congruency = partition_paths_by_congruency(\n",
    "    explainer_predictions=explainer_predictions,\n",
    "    model_predictions=model_predictions,\n",
    "    paths=val_paths)\n",
    "congruent_paths = congruency['congruent']\n",
    "incongruent_paths = congruency['incongruent']\n",
    "print(\"{} congruent paths and {} incongruent paths\".format(\n",
    "    len(congruent_paths), len(incongruent_paths)))\n",
    "\n",
    "# Save to .txt files\n",
    "congruent_paths_save_path = os.path.join(best_output_dir, 'congruent_paths.txt')\n",
    "incongruent_paths_save_path = os.path.join(best_output_dir, 'incongruent_paths.txt')\n",
    "write_lists(congruent_paths, congruent_paths_save_path)\n",
    "write_lists(incongruent_paths, incongruent_paths_save_path)\n",
    "print(\"Wrote congruent paths to {} and incongruent paths to {}\".format(\n",
    "    congruent_paths_save_path, incongruent_paths_save_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89df301-6de8-4ca7-9bc6-6b265336665b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-correlation",
   "language": "python",
   "name": "model-correlation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
