{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "from utils.places365_pred_utils import get_class_category_dict, get_category_class_dict\n",
    "from utils.utils import ensure_dir, write_lists, informal_log\n",
    "from utils.attribute_utils import get_one_hot_attributes, get_frequent_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os, sys\n",
    "\n",
    "# sys.path.insert(0, 'src')\n",
    "\n",
    "# # def get_attributes_set(data, paths):\n",
    "# #     splits = ['train', 'val', 'test']\n",
    "# #     seen_attributes = set()\n",
    "# #     for split in splits:\n",
    "# #         split_paths = paths[split]\n",
    "# #         for path in split_paths:\n",
    "# #             # Obtain attributes\n",
    "# #             cur_attributes = data['labels'][path]\n",
    "# #             for attr in cur_attributes:\n",
    "# #                 seen_attributes.add(attr)\n",
    "# #     return seen_attributes\n",
    "\n",
    "# def get_one_hot_attributes(data, paths, n_attr, splits=['train', 'val', 'test']):\n",
    "#     attributes = {}\n",
    "#     for split in splits:\n",
    "#         attributes[split] = []\n",
    "#     # attributes = {\n",
    "#     #     'train': [],\n",
    "#     #     'val': [],\n",
    "#     #     'test': []\n",
    "#     # }\n",
    "\n",
    "#     for split in splits:\n",
    "#         split_paths = paths[split]\n",
    "#         print(\"Processing attributes for {} split\".format(split))\n",
    "#         for path in tqdm(split_paths):\n",
    "#             # Obtain attributes and covnvert to one hot\n",
    "#             cur_attributes = data['labels'][path]\n",
    "#             one_hot_attributes = np.zeros(n_attr)\n",
    "#             one_hot_attributes[cur_attributes] = 1\n",
    "#             attributes[split].append(one_hot_attributes)\n",
    "#         attributes[split] = np.stack(attributes[split], axis=0)\n",
    "\n",
    "#     # Print statistics from training\n",
    "#     # train_attributes = attributes['train']\n",
    "#     # counts = np.sum(train_attributes, axis=0)\n",
    "#     # print(\"{} concepts that occur > 150 times in training\".format(len(np.where(counts > 150)[0])))\n",
    "#     # print(\"{} concepts that occur at all in training\".format(len(np.nonzero(counts)[0])))\n",
    "\n",
    "#     return attributes\n",
    "\n",
    "# def get_frequent_attributes(attributes,\n",
    "#                                frequency_threshold=150,\n",
    "#                                splits=['train', 'val', 'test']):\n",
    "#     '''\n",
    "#     Given dictionary of 1-hot encoded attributes, return dictionary of same format with only frequent attributes\n",
    "\n",
    "#     Arg(s):\n",
    "#         attributes : dict[str : np.array]\n",
    "#             keys: split ['train', 'val', 'test']\n",
    "#             values: one-hot encoded attributes\n",
    "#         frequency_threshold : int\n",
    "#             number of occurrences in training for an attribute to be considered 'frequent'\n",
    "#         splits : list[str]\n",
    "#             list of split names to key dictionaries\n",
    "\n",
    "#     Returns:\n",
    "#         freq_attributes_dict : dict[str : np.array]\n",
    "#     '''\n",
    "#     train_counts = np.sum(attributes['train'], axis=0)\n",
    "\n",
    "#     # Obtain one-hot encoding of attributes that exceed frequency threshold\n",
    "#     freq_attributes_one_hot = np.where(train_counts > frequency_threshold, 1, 0)\n",
    "#     # Mask out infrequent attributes\n",
    "#     freq_attributes_dict = {}\n",
    "#     for split in splits:\n",
    "#         cur_attributes = attributes[split]\n",
    "#         freq_attributes = np.where(freq_attributes_one_hot == 1, cur_attributes, 0)\n",
    "\n",
    "#         # Sanity checks\n",
    "#         discarded_attributes_idxs = np.nonzero(np.where(train_counts < frequency_threshold, 1, 0))[0]\n",
    "#         kept_attributes_idxs = np.nonzero(train_counts > frequency_threshold)[0]\n",
    "#         assert (kept_attributes_idxs == np.nonzero(freq_attributes_one_hot)[0]).all()\n",
    "\n",
    "#         zeroed_ctr = 0\n",
    "#         ctr = 0\n",
    "\n",
    "#         for idx, (orig, new) in enumerate(zip(cur_attributes, freq_attributes)):\n",
    "#             # print(orig\n",
    "#             if not (orig == new).all():\n",
    "#                 orig_idxs = np.nonzero(orig)[0]\n",
    "#                 new_idxs = np.nonzero(new)[0]\n",
    "#                 # Assert new idxs ONLY contains the kept attributes and none of discarded\n",
    "#                 assert len(np.intersect1d(new_idxs, discarded_attributes_idxs)) == 0\n",
    "#                 assert len(np.intersect1d(new_idxs, kept_attributes_idxs)) == len(new_idxs)\n",
    "#                 # Assert overlap with original indices is equal to new indices\n",
    "#                 assert (np.intersect1d(orig_idxs, new_idxs) == new_idxs).all()\n",
    "#                 if len(new_idxs) == 0:\n",
    "#                     zeroed_ctr += 1\n",
    "#                 ctr += 1\n",
    "#         print(\"{} examples have no more attributes\".format(zeroed_ctr))\n",
    "#         print(\"{}/{} examples affected\".format(ctr, len(cur_attributes)))\n",
    "#         freq_attributes_dict[split] = freq_attributes\n",
    "\n",
    "#     return freq_attributes_dict, freq_attributes_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining one hot encodings of attributes\n",
      "Processing attributes for train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 13326/13326 [00:00<00:00, 116430.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing attributes for val split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4442/4442 [00:00<00:00, 148595.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing attributes for test split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4442/4442 [00:00<00:00, 148150.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining frequent attributes only\n",
      "6 examples have no more attributes\n",
      "8308/13326 examples affected\n",
      "3 examples have no more attributes\n",
      "2736/4442 examples affected\n",
      "3 examples have no more attributes\n",
      "2804/4442 examples affected\n",
      "Loaded human-readable labels)\n"
     ]
    }
   ],
   "source": [
    "# Load features\n",
    "features_dir = os.path.join('saved', 'ADE20K', '0501_105640')\n",
    "train_features_path = os.path.join(features_dir, 'train_features.pth')\n",
    "val_features_path = os.path.join(features_dir, 'val_features.pth')\n",
    "test_features_path = os.path.join(features_dir, 'test_features.pth')\n",
    "\n",
    "train_features_dict = torch.load(train_features_path)\n",
    "train_features = train_features_dict['features']\n",
    "train_paths = train_features_dict['paths']\n",
    "\n",
    "val_features_dict = torch.load(val_features_path)\n",
    "val_features = val_features_dict['features']\n",
    "val_paths = val_features_dict['paths']\n",
    "\n",
    "test_features_dict = torch.load(test_features_path)\n",
    "test_features = test_features_dict['features']\n",
    "test_paths = test_features_dict['paths']\n",
    "\n",
    "paths = {\n",
    "    'train': train_paths,\n",
    "    'val': val_paths,\n",
    "    'test': test_paths\n",
    "}\n",
    "n_attributes = 1200\n",
    "frequency_threshold = 150\n",
    "\n",
    "# Load data and calculate attributes\n",
    "data_path = os.path.join('data', 'ade20k', 'full_ade20k_imagelabels.pth')\n",
    "data = torch.load(data_path)\n",
    "\n",
    "print(\"Obtaining one hot encodings of attributes\")\n",
    "attributes = get_one_hot_attributes(\n",
    "    data=data,\n",
    "    paths=paths,\n",
    "    n_attr=n_attributes\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Obtaining frequent attributes only\")\n",
    "freq_attributes, freq_attributes_one_hot = get_frequent_attributes(\n",
    "    attributes=attributes,\n",
    "    frequency_threshold=frequency_threshold\n",
    ")\n",
    "\n",
    "# Get indices of frequent attributes\n",
    "frequent_attribute_idxs = np.nonzero(freq_attributes_one_hot)[0]\n",
    "\n",
    "# Load names of attributes\n",
    "labels_path = os.path.join('data', 'broden1_224', 'label.csv')\n",
    "attribute_label_dict = pd.read_csv(labels_path, index_col=0)['name'].to_dict()\n",
    "print(\"Loaded human-readable labels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each attribute, create a linear classifier (hyperparameter search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hyperparam_search_multinomial(train_features,\n",
    "                                  train_labels, \n",
    "                                  val_features, \n",
    "                                  val_labels, \n",
    "                                  regularization,\n",
    "                                  solver,\n",
    "                                  scaler=None,\n",
    "                                  Cs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5],\n",
    "                                  log_path=None):\n",
    "    best_clf = None\n",
    "    best_acc = 0\n",
    "    \n",
    "    if scaler is not None:\n",
    "        scaler.fit(train_features)\n",
    "        print(\"Scaler parameters: {}\".format(scaler.get_params()))\n",
    "        train_features = scaler.transform(train_features)\n",
    "        val_features = scaler.transform(val_features)\n",
    "    for c in Cs:\n",
    "        clf = LogisticRegression(solver=solver, C=c, penalty=regularization)\n",
    "        clf.fit(train_features, train_labels)\n",
    "        score = clf.score(val_features, val_labels)\n",
    "        if score>best_acc:\n",
    "            best_acc = score\n",
    "            best_clf = clf\n",
    "            informal_log(\"Best accuracy: {} Regularization: {}\".format(score, c), log_path)\n",
    "    \n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0513_094421] 1/182 Calculating CAV for wall\n",
      "Scaler parameters: {'copy': True, 'with_mean': True, 'with_std': True}\n",
      "Best accuracy: 0.8973435389464205 Regularization: 0.001\n",
      "Best accuracy: 0.8995947771274201 Regularization: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:21, 21.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for wall concept (12): 0.7418\n",
      "[0513_094443] 2/182 Calculating CAV for sky\n",
      "Scaler parameters: {'copy': True, 'with_mean': True, 'with_std': True}\n"
     ]
    }
   ],
   "source": [
    "cavs = {}\n",
    "train_attributes = freq_attributes['train']\n",
    "val_attributes = freq_attributes['val']\n",
    "\n",
    "cavs_save_dir = os.path.join('saved', 'ADE20K', 'cav', 'scaled', datetime.now().strftime(r'%m%d_%H%M%S'))\n",
    "if os.path.exists(cavs_save_dir):\n",
    "    raise ValueError(\"Path {} already exists\".format(cavs_save_dir))\n",
    "ensure_dir(cavs_save_dir)\n",
    "\n",
    "cavs_save_path = os.path.join(cavs_save_dir, 'cavs.pickle')\n",
    "log_path = os.path.join(cavs_save_dir, 'log.txt')\n",
    "n_frequent_attributes = len(frequent_attribute_idxs)\n",
    "\n",
    "for idx, attribute_idx in tqdm(enumerate(frequent_attribute_idxs)):\n",
    "    informal_log(\"[{}] {}/{} Calculating CAV for {}\".format(\n",
    "        datetime.now().strftime(r'%m%d_%H%M%S'),\n",
    "        idx+1,\n",
    "        n_frequent_attributes,\n",
    "        attribute_label_dict[attribute_idx]), log_path)\n",
    "    scaler = Preprocessing.StandardScaler()\n",
    "    cav = hyperparam_search_multinomial(\n",
    "        train_features=train_features,\n",
    "        train_labels=train_attributes[:, attribute_idx],\n",
    "        val_features=val_features,\n",
    "        val_labels=val_attributes[:, attribute_idx],\n",
    "        scaler=scaler,\n",
    "        solver='liblinear',\n",
    "        regularization='l2',\n",
    "        log_path=log_path\n",
    "    )\n",
    "    cavs[attribute_idx] = cav\n",
    "    \n",
    "    accuracy = cav.score(val_features, val_attributes[:, attribute_idx])\n",
    "    informal_log(\"CAV accuracy for {} concept ({}): {:.4f}\".format(\n",
    "        attribute_label_dict[attribute_idx],\n",
    "        attribute_idx,\n",
    "        accuracy), log_path)\n",
    "    if idx % 10 == 0:\n",
    "        pickle.dump(cavs, open(cavs_save_path, 'wb'))\n",
    "        \n",
    "pickle.dump(cavs, open(cavs_save_path, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5051778478162989\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Have CAVs trained using ADE20K training set\n",
    "For each split:\n",
    "\n",
    "'''\n",
    "\n",
    "concept_vectors_dict = {}\n",
    "concept_vectors_save_path = os.path.join(cavs_save_dir, '{}_cavs.pth')\n",
    "for split in splits:\n",
    "    split_features = features[split]\n",
    "    concept_vectors = []\n",
    "    for cav in cavs: # 182 of these\n",
    "        concept_present = cav.predict(split_features) # N x 1 array\n",
    "        concept_vectors.append(concept_present)\n",
    "    concept_vectors = np.stack(concept_vectors, axis=1)\n",
    "    concept_vectors_dict[split] = concept_vectors\n",
    "    torch.save(concept_vectors, concept_vectors_save_path.format(split))\n",
    "\n",
    "# Load model's predictions\n",
    "# predictions_path = os.path.join(features_dir, '{}_logits_predictions.pth')\n",
    "# train_predictions = torch.load(predictions_path.format('train'))['predictions']\n",
    "# val_predictions = torch.load(predictions_path.format('val'))['predictions']\n",
    "# test_predictions = torch.load(predictions_path.format('test'))['predictions']\n",
    "\n",
    "# predictions = {\n",
    "#     'train': train_predictions,\n",
    "#     'val': val_predictions, \n",
    "#     'test': test_predictions\n",
    "# }\n",
    "prediction_path = os.path.join('saved', 'PlacesCategoryClassification', 'saga', '{}_outputs_predictions.pth')\n",
    "splits = ['train', 'val', 'test']\n",
    "predictions = {}\n",
    "for split in splits:\n",
    "    predictions[split] = torch.load(prediction_path.format(split))['predictions']\n",
    "\n",
    "solver = 'saga'\n",
    "penalty = 'l1',\n",
    "max_iter = 200\n",
    "hyperparam_search_multinomial(\n",
    "    train_features=# NEED TO FILL IN, calculate concept vectors for all inputs,\n",
    "    train_labels=predictions['train'], \n",
    "    val_features=#NEED TO FILL IN, claculate concept vectors for all inputs, \n",
    "    val_labels=predictions['val'], \n",
    "    regularization=penalty,\n",
    "    solver=solver,\n",
    "    scaler=None,\n",
    "    Cs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5],\n",
    "    log_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-correlation",
   "language": "python",
   "name": "model-correlation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c579736b944d6d0768ad80dbc3e033126adf2082032258477813cfb04cf1061b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
