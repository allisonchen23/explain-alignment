{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'partition_paths_by_congruency' from 'utils.attribute_utils' (/n/fs/ac-alignment/explain-alignment/src/utils/attribute_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplaces365_pred_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_class_category_dict, get_category_class_dict\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m ensure_dir, write_lists, informal_log\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattribute_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_one_hot_attributes, get_frequent_attributes, hyperparam_search, partition_paths_by_congruency\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'partition_paths_by_congruency' from 'utils.attribute_utils' (/n/fs/ac-alignment/explain-alignment/src/utils/attribute_utils.py)"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "from utils.places365_pred_utils import get_class_category_dict, get_category_class_dict\n",
    "from utils.utils import ensure_dir, write_lists, informal_log\n",
    "from utils.attribute_utils import get_one_hot_attributes, get_frequent_attributes, hyperparam_search, partition_paths_by_congruency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining one hot encodings of attributes\n",
      "Processing attributes for train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13326/13326 [00:00<00:00, 130250.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing attributes for val split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4442/4442 [00:00<00:00, 157671.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing attributes for test split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4442/4442 [00:00<00:00, 155199.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining frequent attributes only\n",
      "6 examples have no more attributes\n",
      "8308/13326 examples affected\n",
      "3 examples have no more attributes\n",
      "2736/4442 examples affected\n",
      "3 examples have no more attributes\n",
      "2804/4442 examples affected\n",
      "Loaded human-readable labels)\n"
     ]
    }
   ],
   "source": [
    "# Load features\n",
    "features_dir = os.path.join('saved', 'ADE20K', '0501_105640')\n",
    "train_features_path = os.path.join(features_dir, 'train_features.pth')\n",
    "val_features_path = os.path.join(features_dir, 'val_features.pth')\n",
    "test_features_path = os.path.join(features_dir, 'test_features.pth')\n",
    "\n",
    "train_features_dict = torch.load(train_features_path)\n",
    "train_features = train_features_dict['features']\n",
    "train_paths = train_features_dict['paths']\n",
    "\n",
    "val_features_dict = torch.load(val_features_path)\n",
    "val_features = val_features_dict['features']\n",
    "val_paths = val_features_dict['paths']\n",
    "\n",
    "test_features_dict = torch.load(test_features_path)\n",
    "test_features = test_features_dict['features']\n",
    "test_paths = test_features_dict['paths']\n",
    "\n",
    "features = {\n",
    "    'train': train_features,\n",
    "    'val': val_features,\n",
    "    'test': test_features\n",
    "}\n",
    "paths = {\n",
    "    'train': train_paths,\n",
    "    'val': val_paths,\n",
    "    'test': test_paths\n",
    "}\n",
    "n_attributes = 1200\n",
    "frequency_threshold = 150\n",
    "\n",
    "# Load data and calculate attributes\n",
    "data_path = os.path.join('data', 'ade20k', 'full_ade20k_imagelabels.pth')\n",
    "data = torch.load(data_path)\n",
    "\n",
    "print(\"Obtaining one hot encodings of attributes\")\n",
    "attributes = get_one_hot_attributes(\n",
    "    data=data,\n",
    "    paths=paths,\n",
    "    n_attr=n_attributes\n",
    ")\n",
    "attribute_save_path = os.path.join(os.path.dirname(data_path), 'one_hot_attributes.pth')\n",
    "if not os.path.exists(attribute_save_path):\n",
    "    torch.save(attributes, attribute_save_path)\n",
    "    print(\"Saved one hot attributes from train/val/test to {}\".format(attribute_save_path))\n",
    "\n",
    "print(\"Obtaining frequent attributes only\")\n",
    "freq_attributes, freq_attributes_one_hot = get_frequent_attributes(\n",
    "    attributes=attributes,\n",
    "    frequency_threshold=frequency_threshold\n",
    ")\n",
    "\n",
    "# Get indices of frequent attributes\n",
    "frequent_attribute_idxs = np.nonzero(freq_attributes_one_hot)[0]\n",
    "freq_attribute_save_path = os.path.join(os.path.dirname(data_path), 'frequency_filtered_one_hot_attributes.pth')\n",
    "if not os.path.exists(freq_attribute_save_path):\n",
    "    torch.save(freq_attributes, freq_attribute_save_path)\n",
    "    print(\"Saved frequency filtered one hot attributes from train/val/test to {}\".format(freq_attribute_save_path))\n",
    "\n",
    "frequent_idx_save_path = os.path.join(os.path.dirname(data_path), 'frequent_attribute_idxs.pth')\n",
    "if not os.path.exists(frequent_idx_save_path):\n",
    "    torch.save(frequent_attribute_idxs, frequent_idx_save_path)\n",
    "    print(\"Saved indices of frequent attributes from training to {}\".format(frequent_idx_save_path))\n",
    "# Load names of attributes\n",
    "labels_path = os.path.join('data', 'broden1_224', 'label.csv')\n",
    "attribute_label_dict = pd.read_csv(labels_path, index_col=0)['name'].to_dict()\n",
    "print(\"Loaded human-readable labels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each attribute, create a linear classifier (hyperparameter search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def hyperparam_search(train_features,\n",
    "#                                   train_labels, \n",
    "#                                   val_features, \n",
    "#                                   val_labels, \n",
    "#                                   regularization,\n",
    "#                                   solver,\n",
    "#                                   scaler=None,\n",
    "#                                   Cs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5],\n",
    "#                                   log_path=None):\n",
    "#     best_clf = None\n",
    "#     best_acc = 0\n",
    "    \n",
    "#     if scaler is not None:\n",
    "#         scaler.fit(train_features)\n",
    "#         print(\"Scaler parameters: {}\".format(scaler.get_params()))\n",
    "#         train_features = scaler.transform(train_features)\n",
    "#         val_features = scaler.transform(val_features)\n",
    "#     for c in Cs:\n",
    "#         clf = LogisticRegression(solver=solver, C=c, penalty=regularization)\n",
    "#         clf.fit(train_features, train_labels)\n",
    "#         score = clf.score(val_features, val_labels)\n",
    "#         if score>best_acc:\n",
    "#             best_acc = score\n",
    "#             best_clf = clf\n",
    "#             informal_log(\"Best accuracy: {} Regularization: {}\".format(score, c), log_path)\n",
    "    \n",
    "#     return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0515_142919] 1/182 Calculating CAV for wall\n",
      "Best accuracy: 0.8973435389464205 Regularization: 0.001\n",
      "Best accuracy: 0.8975686627645205 Regularization: 0.005\n",
      "Best accuracy: 0.8982440342188204 Regularization: 0.01\n",
      "Best accuracy: 0.8991445294912201 Regularization: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:23, 23.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for wall concept (12): 0.8991\n",
      "[0515_142942] 2/182 Calculating CAV for sky\n",
      "Best accuracy: 0.9279603782080144 Regularization: 0.001\n",
      "Best accuracy: 0.935164340387213 Regularization: 0.005\n",
      "Best accuracy: 0.9371904547501125 Regularization: 0.01\n",
      "Best accuracy: 0.9425934263845115 Regularization: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:45, 22.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for sky concept (13): 0.9426\n",
      "[0515_143004] 3/182 Calculating CAV for floor\n",
      "Best accuracy: 0.9189554254840162 Regularization: 0.001\n",
      "Best accuracy: 0.9236830256641153 Regularization: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:09, 23.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for floor concept (14): 0.9237\n",
      "[0515_143028] 4/182 Calculating CAV for windowpane\n",
      "Best accuracy: 0.7976136875281404 Regularization: 0.001\n",
      "Best accuracy: 0.8077442593426385 Regularization: 0.005\n",
      "Best accuracy: 0.8108959927960379 Regularization: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:35, 24.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for windowpane concept (15): 0.8109\n",
      "[0515_143054] 5/182 Calculating CAV for tree\n",
      "Best accuracy: 0.9040972534894192 Regularization: 0.001\n",
      "Best accuracy: 0.909725348941918 Regularization: 0.005\n",
      "Best accuracy: 0.9106258442143179 Regularization: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [02:01, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for tree concept (16): 0.9106\n",
      "[0515_143120] 6/182 Calculating CAV for building\n",
      "Best accuracy: 0.9108509680324178 Regularization: 0.001\n",
      "Best accuracy: 0.9153534443944169 Regularization: 0.005\n",
      "Best accuracy: 0.9164790634849167 Regularization: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [02:07, 25.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m informal_log(\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m Calculating CAV for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     16\u001b[0m     datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     17\u001b[0m     idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     18\u001b[0m     n_frequent_attributes,\n\u001b[1;32m     19\u001b[0m     attribute_label_dict[attribute_idx]), log_path)\n\u001b[1;32m     20\u001b[0m scaler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m# Preprocessing.StandardScaler()\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m cav \u001b[39m=\u001b[39m hyperparam_search(\n\u001b[1;32m     22\u001b[0m     train_features\u001b[39m=\u001b[39;49mtrain_features,\n\u001b[1;32m     23\u001b[0m     train_labels\u001b[39m=\u001b[39;49mtrain_attributes[:, attribute_idx],\n\u001b[1;32m     24\u001b[0m     val_features\u001b[39m=\u001b[39;49mval_features,\n\u001b[1;32m     25\u001b[0m     val_labels\u001b[39m=\u001b[39;49mval_attributes[:, attribute_idx],\n\u001b[1;32m     26\u001b[0m     scaler\u001b[39m=\u001b[39;49mscaler,\n\u001b[1;32m     27\u001b[0m     solver\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m     regularization\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ml2\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m     log_path\u001b[39m=\u001b[39;49mlog_path\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m cavs[attribute_idx] \u001b[39m=\u001b[39m cav\n\u001b[1;32m     33\u001b[0m accuracy \u001b[39m=\u001b[39m cav\u001b[39m.\u001b[39mscore(val_features, val_attributes[:, attribute_idx])\n",
      "File \u001b[0;32m/n/fs/ac-alignment/explain-alignment/src/utils/attribute_utils.py:117\u001b[0m, in \u001b[0;36mhyperparam_search\u001b[0;34m(train_features, train_labels, val_features, val_labels, regularization, solver, scaler, Cs, log_path)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m Cs:\n\u001b[1;32m    116\u001b[0m     clf \u001b[39m=\u001b[39m LogisticRegression(solver\u001b[39m=\u001b[39msolver, C\u001b[39m=\u001b[39mc, penalty\u001b[39m=\u001b[39mregularization)\n\u001b[0;32m--> 117\u001b[0m     clf\u001b[39m.\u001b[39;49mfit(train_features, train_labels)\n\u001b[1;32m    118\u001b[0m     score \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mscore(val_features, val_labels)\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m score\u001b[39m>\u001b[39mbest_acc:\n",
      "File \u001b[0;32m/n/fs/ac-project/anaconda3/envs/model-correlation/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1216\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[39mif\u001b[39;00m effective_n_jobs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1211\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1212\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m > 1 does not have any effect when\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39msolver\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is set to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Got \u001b[39m\u001b[39m'\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(effective_n_jobs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs))\n\u001b[1;32m   1215\u001b[0m         )\n\u001b[0;32m-> 1216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m _fit_liblinear(\n\u001b[1;32m   1217\u001b[0m         X,\n\u001b[1;32m   1218\u001b[0m         y,\n\u001b[1;32m   1219\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[1;32m   1220\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m   1221\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintercept_scaling,\n\u001b[1;32m   1222\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m   1223\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpenalty,\n\u001b[1;32m   1224\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdual,\n\u001b[1;32m   1225\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1226\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1227\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m   1228\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m   1229\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1230\u001b[0m     )\n\u001b[1;32m   1231\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m   1233\u001b[0m \u001b[39mif\u001b[39;00m solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39msag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m/n/fs/ac-project/anaconda3/envs/model-correlation/lib/python3.11/site-packages/sklearn/svm/_base.py:1224\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1221\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m   1223\u001b[0m solver_type \u001b[39m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1224\u001b[0m raw_coef_, n_iter_ \u001b[39m=\u001b[39m liblinear\u001b[39m.\u001b[39;49mtrain_wrap(\n\u001b[1;32m   1225\u001b[0m     X,\n\u001b[1;32m   1226\u001b[0m     y_ind,\n\u001b[1;32m   1227\u001b[0m     sp\u001b[39m.\u001b[39;49misspmatrix(X),\n\u001b[1;32m   1228\u001b[0m     solver_type,\n\u001b[1;32m   1229\u001b[0m     tol,\n\u001b[1;32m   1230\u001b[0m     bias,\n\u001b[1;32m   1231\u001b[0m     C,\n\u001b[1;32m   1232\u001b[0m     class_weight_,\n\u001b[1;32m   1233\u001b[0m     max_iter,\n\u001b[1;32m   1234\u001b[0m     rnd\u001b[39m.\u001b[39;49mrandint(np\u001b[39m.\u001b[39;49miinfo(\u001b[39m\"\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmax),\n\u001b[1;32m   1235\u001b[0m     epsilon,\n\u001b[1;32m   1236\u001b[0m     sample_weight,\n\u001b[1;32m   1237\u001b[0m )\n\u001b[1;32m   1238\u001b[0m \u001b[39m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[39m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[39m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[39m# srand supports\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m n_iter_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cavs = {}\n",
    "train_attributes = freq_attributes['train']\n",
    "val_attributes = freq_attributes['val']\n",
    "\n",
    "cavs_save_dir = os.path.join('saved', 'ADE20K', 'cav', datetime.now().strftime(r'%m%d_%H%M%S'))\n",
    "if os.path.exists(cavs_save_dir):\n",
    "    raise ValueError(\"Path {} already exists\".format(cavs_save_dir))\n",
    "ensure_dir(cavs_save_dir)\n",
    "\n",
    "cavs_save_path = os.path.join(cavs_save_dir, 'cavs.pickle')\n",
    "log_path = os.path.join(cavs_save_dir, 'log.txt')\n",
    "n_frequent_attributes = len(frequent_attribute_idxs)\n",
    "\n",
    "for idx, attribute_idx in tqdm(enumerate(frequent_attribute_idxs)):\n",
    "    informal_log(\"[{}] {}/{} Calculating CAV for {}\".format(\n",
    "        datetime.now().strftime(r'%m%d_%H%M%S'),\n",
    "        idx+1,\n",
    "        n_frequent_attributes,\n",
    "        attribute_label_dict[attribute_idx]), log_path)\n",
    "    scaler = None # Preprocessing.StandardScaler()\n",
    "    cav = hyperparam_search(\n",
    "        train_features=train_features,\n",
    "        train_labels=train_attributes[:, attribute_idx],\n",
    "        val_features=val_features,\n",
    "        val_labels=val_attributes[:, attribute_idx],\n",
    "        scaler=scaler,\n",
    "        solver='liblinear',\n",
    "        regularization='l2',\n",
    "        log_path=log_path\n",
    "    )\n",
    "    cavs[attribute_idx] = cav\n",
    "    \n",
    "    accuracy = cav.score(val_features, val_attributes[:, attribute_idx])\n",
    "    informal_log(\"CAV accuracy for {} concept ({}): {:.4f}\".format(\n",
    "        attribute_label_dict[attribute_idx],\n",
    "        attribute_idx,\n",
    "        accuracy), log_path)\n",
    "    # Save periodically\n",
    "    if idx % 10 == 0:\n",
    "        pickle.dump(cavs, open(cavs_save_path, 'wb'))\n",
    "        \n",
    "pickle.dump(cavs, open(cavs_save_path, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept presence vectors already exist at saved/ADE20K/cav/scaled/0513_094421/cav_attributes.pth\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Have CAVs trained using ADE20K training set\n",
    "For each split:\n",
    "\n",
    "'''\n",
    "cavs_path = 'saved/ADE20K/cav/scaled/0513_094421/cavs.pickle'\n",
    "cavs_save_dir = os.path.dirname(cavs_path)\n",
    "if 'scaled' in cavs_path:\n",
    "    scale = True\n",
    "else:\n",
    "    scale = False\n",
    "cavs = pickle.load(open(cavs_path, 'rb'))\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "if scale:\n",
    "    import sklearn.preprocessing as Preprocessing\n",
    "    scaler = Preprocessing.StandardScaler()\n",
    "    scaler.fit(features['train'])\n",
    "    scaled_features = {\n",
    "        'train': scaler.transform(features['train']),\n",
    "        'val': scaler.transform(features['val']),\n",
    "        'test': scaler.transform(features['test'])\n",
    "    }\n",
    "concept_vectors_dict = {}\n",
    "concept_vectors_save_path = os.path.join(cavs_save_dir, 'cav_attributes.pth')\n",
    "if os.path.exists(concept_vectors_save_path):\n",
    "    print(\"Concept presence vectors already exist at {}\".format(concept_vectors_save_path))\n",
    "else:\n",
    "    for split in splits:\n",
    "        if scale:\n",
    "            split_features = scaled_features[split]\n",
    "        else:\n",
    "            split_features = features[split]\n",
    "        n_samples = len(split_features)\n",
    "        print(\"Obtaining concept presence vectors for {} split\".format(split))\n",
    "\n",
    "        concept_presence_vectors = []\n",
    "        for attr_idx in tqdm(range(n_attributes)):\n",
    "            if attr_idx in cavs:\n",
    "                cav = cavs[attr_idx]\n",
    "                concept_present = cav.predict(split_features)\n",
    "                concept_presence_vectors.append(concept_present)\n",
    "                assert len(concept_present) == n_samples\n",
    "            else:\n",
    "                concept_presence_vectors.append(np.zeros(n_samples))\n",
    "        concept_presence_vectors = np.stack(concept_presence_vectors, axis=1)\n",
    "        print(concept_presence_vectors.shape, n_samples)\n",
    "        # Concept vectors only for frequent vectors; Turn it into one hot vector\n",
    "\n",
    "        concept_vectors_dict[split] = concept_presence_vectors\n",
    "        \n",
    "    torch.save(concept_vectors_dict, concept_vectors_save_path)\n",
    "    print(\"Saved concept present vectors from CAVs to {}\".format(concept_vectors_save_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cpvs\n",
    "cav_attributes_path = 'saved/ADE20K/cav/scaled/0513_094421/cav_attributes.pth'\n",
    "cav_attributes = torch.load(cav_attributes_path)\n",
    "\n",
    "# Load model's predictions\n",
    "# predictions_path = os.path.join(features_dir, '{}_logits_predictions.pth')\n",
    "# train_predictions = torch.load(predictions_path.format('train'))['predictions']\n",
    "# val_predictions = torch.load(predictions_path.format('val'))['predictions']\n",
    "# test_predictions = torch.load(predictions_path.format('test'))['predictions']\n",
    "\n",
    "# predictions = {\n",
    "#     'train': train_predictions,\n",
    "#     'val': val_predictions, \n",
    "#     'test': test_predictions\n",
    "# }\n",
    "\n",
    "prediction_path = os.path.join(\n",
    "    'saved', \n",
    "    'PlacesCategoryClassification',\n",
    "    '0510_102912',\n",
    "    'ADE20K_predictions', \n",
    "    'saga', \n",
    "    '{}_outputs_predictions.pth')\n",
    "splits = ['train', 'val', 'test']\n",
    "predictions = {}\n",
    "for split in splits:\n",
    "    predictions[split] = torch.load(prediction_path.format(split))['predictions']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train explainer\n",
    "solver = 'saga'\n",
    "penalty = 'l1'\n",
    "c = 0.5\n",
    "max_iter = 200\n",
    "# hyperparam_search(\n",
    "#     train_features=cav_attributes['train'],\n",
    "#     train_labels=predictions['train'], \n",
    "#     val_features=cav_attributes['val'], \n",
    "#     val_labels=predictions['val'], \n",
    "#     regularization=penalty,\n",
    "#     solver=solver,\n",
    "#     scaler=None,\n",
    "#     Cs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5],\n",
    "#     log_path=None)\n",
    "explainer = LogisticRegression(\n",
    "    solver=solver,\n",
    "    penalty=penalty,\n",
    "    C=c,\n",
    "    multi_class='multinomial',\n",
    "    max_iter=max_iter\n",
    ")\n",
    "explainer.fit(cav_attributes['train'], predictions['train'])\n",
    "accuracy = explainer.score(cav_attributes['val'], predictions['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_paths_by_congruency(explainer_predictions,\n",
    "                                  model_predictions,\n",
    "                                  paths):\n",
    "    '''\n",
    "    Given list or arrays of explainer and model predictions, partition paths based on if predictions align\n",
    "\n",
    "    Arg(s):\n",
    "        explainer_predictions : N-length np.array\n",
    "            predictions output by the explainer model\n",
    "        model_predictions : N-length np.array\n",
    "            predictions output by the model\n",
    "        paths : N-length list\n",
    "            paths of images corresponding to each data point\n",
    "\n",
    "    Returns:\n",
    "        dictionary : dict[str] : list\n",
    "            key: 'congruent' or 'incongruent'\n",
    "            value: list of paths\n",
    "    '''\n",
    "    n_samples = len(paths)\n",
    "    assert len(explainer_predictions) == n_samples\n",
    "    assert len(model_predictions) == n_samples, \"Length of model predictions {} doesn't match n_samples {}\".format(\n",
    "        len(model_predictions), n_samples\n",
    "    )\n",
    "\n",
    "    incongruent_paths = []\n",
    "    congruent_paths = []\n",
    "\n",
    "    for explainer_prediction, model_prediction, path in tqdm(zip(\n",
    "        explainer_predictions, model_predictions, paths\n",
    "    )):\n",
    "        if explainer_prediction == model_prediction:\n",
    "            congruent_paths.append(path)\n",
    "        else:\n",
    "            incongruent_paths.append(path)\n",
    "\n",
    "    return {\n",
    "        'congruent': congruent_paths,\n",
    "        'incongruent': incongruent_paths\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (530267873.py, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 53\u001b[0;36m\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_dir = os.path.dirname(prediction_path)\n",
    "\n",
    "# Save CAV explainer\n",
    "cav_explainer_dir = os.path.join(save_dir, 'cav_explainer')\n",
    "ensure_dir(cav_explainer_dir)\n",
    "cav_explainer_save_path = os.path.join(cav_explainer_dir, '{}_explainer_{}_{}.pickle'.format(solver, penalty, c))\n",
    "if not os.path.exists(cav_explainer_save_path):\n",
    "    pickle.dump(explainer, open(cav_explainer_save_path, 'wb'))\n",
    "    print(\"Saved explainer trained on CAVs to {}\".format(cav_explainer_save_path))\n",
    "else:\n",
    "    print(\"Explainer already exists at '{}'\".format(cav_explainer_save_path))\n",
    "\n",
    "# Save CAV explainer predictions\n",
    "accuracy = explainer.score(cav_attributes['val'], predictions['val'])\n",
    "print(accuracy)\n",
    "\n",
    "explainer_outputs = explainer.decision_function(cav_attributes['val'])\n",
    "explainer_probabilities = explainer.predict_proba(cav_attributes['val'])\n",
    "explainer_predictions = explainer.predict(cav_attributes['val'])\n",
    "\n",
    "validation_output = {\n",
    "    'outputs': explainer_outputs,\n",
    "    'probabilities': explainer_probabilities,\n",
    "    'predictions': explainer_predictions\n",
    "}\n",
    "validation_output_path = os.path.join(cav_explainer_dir, '{}_explainer_{}_{}_validation.pth'.format(solver, penalty, c))\n",
    "if not os.path.exists(validation_output_path):\n",
    "    torch.save(validation_output, validation_output_path)\n",
    "    print(\"Saved outputs from validation set to {}\".format(validation_output_path))\n",
    "else:\n",
    "    print(\"Validation set outputs already saved to {}\".format(validation_output_path))\n",
    "\n",
    "# Save congruent/incongruent paths\n",
    "congruency_paths = partition_paths_by_congruency(\n",
    "    explainer_predictions=explainer_predictions,\n",
    "    model_predictions=predictions['val'],\n",
    "    paths=paths['val']\n",
    ")\n",
    "congruent_paths = congruency_paths['congruent']\n",
    "incongruent_paths = congruency_paths['incongruent']\n",
    "print(len(congruent_paths), len(incongruent_paths))\n",
    "congruent_paths_path = os.path.join(save_dir, 'congruent_paths.txt')\n",
    "incongruent_paths_path = os.path.join(save_dir, 'incongruent_paths.txt')\n",
    "if not os.path.exists(congruent_paths_path) or not os.path.exists(incongruent_paths_path):\n",
    "    write_lists(congruent_paths, congruent_paths_path)\n",
    "    write_lists(incongruent_paths, incongruent_paths_path)\n",
    "    print(\"Saved {} congruent paths to {} and {} incongruent paths to {}\".format(\n",
    "        len(congruent_paths),\n",
    "        congruent_paths_path,\n",
    "        len(incongruent_paths),\n",
    "        incongruent_paths_path\n",
    "    ))\n",
    "else:\n",
    "    print(\"Congruent paths already saved to {} and incongruent paths already saved to {}\".format(\n",
    "        congruent_paths_path, incongruent_paths_path\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 ('model-correlation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c579736b944d6d0768ad80dbc3e033126adf2082032258477813cfb04cf1061b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
