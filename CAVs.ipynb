{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "from utils.places365_pred_utils import get_class_category_dict, get_category_class_dict\n",
    "from utils.utils import ensure_dir, write_lists, informal_log\n",
    "from utils.attribute_utils import get_one_hot_attributes, get_frequent_attributes, hyperparam_search, partition_paths_by_congruency, convert_sparse_to_dense_attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining one hot encodings of attributes\n",
      "Processing attributes for train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 13326/13326 [00:00<00:00, 94009.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing attributes for val split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 4442/4442 [00:00<00:00, 79593.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing attributes for test split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 4442/4442 [00:00<00:00, 62211.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining frequent attributes only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 examples have no more attributes\n",
      "0/13326 examples affected\n",
      "0 examples have no more attributes\n",
      "0/4442 examples affected\n",
      "0 examples have no more attributes\n",
      "2/4442 examples affected\n",
      "After filtering concepts that appear in 0+ images, we have 623 concepts\n",
      "Path data/ade20k/filter_attr_0/splits_sparse_one_hot_attributes.pth already exists\n",
      "Path to data/ade20k/filter_attr_0/splits_dense_one_hot_attributes.pth already exists\n",
      "Loaded human-readable labels)\n"
     ]
    }
   ],
   "source": [
    "# Load features\n",
    "features_dir = os.path.join('saved', 'ADE20K', '0501_105640')\n",
    "train_features_path = os.path.join(features_dir, 'train_features.pth')\n",
    "val_features_path = os.path.join(features_dir, 'val_features.pth')\n",
    "test_features_path = os.path.join(features_dir, 'test_features.pth')\n",
    "\n",
    "train_features_dict = torch.load(train_features_path)\n",
    "train_features = train_features_dict['features']\n",
    "train_paths = train_features_dict['paths']\n",
    "\n",
    "val_features_dict = torch.load(val_features_path)\n",
    "val_features = val_features_dict['features']\n",
    "val_paths = val_features_dict['paths']\n",
    "\n",
    "test_features_dict = torch.load(test_features_path)\n",
    "test_features = test_features_dict['features']\n",
    "test_paths = test_features_dict['paths']\n",
    "\n",
    "features = {\n",
    "    'train': train_features,\n",
    "    'val': val_features,\n",
    "    'test': test_features\n",
    "}\n",
    "paths = {\n",
    "    'train': train_paths,\n",
    "    'val': val_paths,\n",
    "    'test': test_paths\n",
    "}\n",
    "n_attributes = 1200\n",
    "frequency_threshold = 0\n",
    "\n",
    "# Load data and calculate attributes\n",
    "data_path = os.path.join('data', 'ade20k', 'full_ade20k_imagelabels.pth')\n",
    "data = torch.load(data_path)\n",
    "\n",
    "print(\"Obtaining one hot encodings of attributes\")\n",
    "attributes = get_one_hot_attributes(\n",
    "    data=data,\n",
    "    paths=paths,\n",
    "    n_attr=n_attributes\n",
    ")\n",
    "attribute_save_path = os.path.join(os.path.dirname(data_path), 'one_hot_attributes.pth')\n",
    "if not os.path.exists(attribute_save_path):\n",
    "    torch.save(attributes, attribute_save_path)\n",
    "    print(\"Saved one hot attributes from train/val/test to {}\".format(attribute_save_path))\n",
    "\n",
    "print(\"Obtaining frequent attributes only\")\n",
    "sparse_freq_attributes, metadata = get_frequent_attributes(\n",
    "    attributes=attributes,\n",
    "    frequency_threshold=frequency_threshold\n",
    ")\n",
    "\n",
    "# Get indices of frequent attributes\n",
    "sparse_freq_attributes_one_hot = metadata['freq_attr_one_hot']\n",
    "frequent_attribute_idxs = metadata['freq_attr_idxs']\n",
    "\n",
    "print(\"After filtering concepts that appear in {}+ images, we have {} concepts\".format(\n",
    "    frequency_threshold, len(frequent_attribute_idxs)))\n",
    "      \n",
    "# Get dense attributes\n",
    "dense_attributes = {}\n",
    "for split in sparse_freq_attributes.keys():\n",
    "    dense_attributes[split] = convert_sparse_to_dense_attributes(\n",
    "        sparse_attributes=sparse_freq_attributes[split],\n",
    "        used_attributes_idxs=frequent_attribute_idxs)\n",
    "    \n",
    "# Save dir for this frequency threshold\n",
    "save_dir = os.path.join(os.path.dirname(data_path), 'filter_attr_{}'.format(frequency_threshold))\n",
    "ensure_dir(save_dir)\n",
    "# Save sparse attributes from each split\n",
    "sparse_freq_attribute_save_path = os.path.join(save_dir, 'splits_sparse_one_hot_attributes.pth')\n",
    "if not os.path.exists(sparse_freq_attribute_save_path):\n",
    "    torch.save(sparse_freq_attributes, sparse_freq_attribute_save_path)\n",
    "    print(\"Saved frequency filtered one hot attributes from train/val/test to {}\".format(sparse_freq_attribute_save_path))\n",
    "else:\n",
    "    print(\"Path {} already exists\".format(sparse_freq_attribute_save_path))\n",
    "    \n",
    "# Save dense attributes from each split\n",
    "dense_freq_attributes_save_path = os.path.join(save_dir, 'splits_dense_one_hot_attributes.pth')\n",
    "if not os.path.exists(dense_freq_attributes_save_path):\n",
    "    torch.save(dense_attributes, dense_freq_attributes_save_path)\n",
    "    print(\"Saved dense frequency filtered one hot attributes from train/val/test to {}\".format(dense_freq_attributes_save_path))\n",
    "else:\n",
    "    print(\"Path to {} already exists\".format(dense_freq_attributes_save_path))\n",
    "# Save the idxs of the attributes that were kept after filtering for frequency\n",
    "frequent_idx_save_path = os.path.join(save_dir, 'frequent_attribute_idxs_n-{}.pth'.format(len(frequent_attribute_idxs)))\n",
    "if not os.path.exists(frequent_idx_save_path):\n",
    "    torch.save(frequent_attribute_idxs, frequent_idx_save_path)\n",
    "    print(\"Saved indices of frequent attributes from training to {}\".format(frequent_idx_save_path))\n",
    "# Load names of attributes\n",
    "labels_path = os.path.join('data', 'broden1_224', 'label.csv')\n",
    "attribute_label_dict = pd.read_csv(labels_path, index_col=0)['name'].to_dict()\n",
    "print(\"Loaded human-readable labels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each attribute, create a linear classifier (hyperparameter search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13326, 623)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0804_105359] 1/623 Calculating CAV for wall\n",
      "Best accuracy: 0.8986942818550203 Regularization: 0.001\n",
      "Best accuracy: 0.9004952723998199 Regularization: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:22, 22.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for wall concept (12): 0.9005\n",
      "[0804_105422] 2/623 Calculating CAV for sky\n",
      "Best accuracy: 0.9268347591175147 Regularization: 0.001\n",
      "Best accuracy: 0.9342638451148132 Regularization: 0.005\n",
      "Best accuracy: 0.9376407023863125 Regularization: 0.01\n",
      "Best accuracy: 0.9398919405673121 Regularization: 0.05\n",
      "Best accuracy: 0.9410175596578118 Regularization: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:43, 21.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for sky concept (13): 0.9410\n",
      "[0804_105442] 3/623 Calculating CAV for floor\n",
      "Best accuracy: 0.9207564160288159 Regularization: 0.001\n",
      "Best accuracy: 0.9218820351193157 Regularization: 0.005\n",
      "Best accuracy: 0.9225574065736155 Regularization: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:05, 21.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for floor concept (14): 0.9226\n",
      "[0804_105504] 4/623 Calculating CAV for windowpane\n",
      "Best accuracy: 0.7973885637100405 Regularization: 0.001\n",
      "Best accuracy: 0.8057181449797388 Regularization: 0.005\n",
      "Best accuracy: 0.8059432687978388 Regularization: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:30, 22.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for windowpane concept (15): 0.8059\n",
      "[0804_105529] 5/623 Calculating CAV for tree\n",
      "Best accuracy: 0.8818099954975236 Regularization: 0.001\n",
      "Best accuracy: 0.895092300765421 Regularization: 0.005\n",
      "Best accuracy: 0.8980189104007203 Regularization: 0.01\n",
      "Best accuracy: 0.9022962629446195 Regularization: 0.05\n",
      "Best accuracy: 0.9029716343989194 Regularization: 0.1\n",
      "Best accuracy: 0.9036470058532192 Regularization: 0.5\n",
      "Best accuracy: 0.9038721296713192 Regularization: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:53, 23.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for tree concept (16): 0.9039\n",
      "[0804_105553] 6/623 Calculating CAV for wood\n",
      "Best accuracy: 0.9529491220171095 Regularization: 0.001\n",
      "Best accuracy: 0.975461503827105 Regularization: 0.005\n",
      "Best accuracy: 0.979963980189104 Regularization: 0.01\n",
      "Best accuracy: 0.9909950472760019 Regularization: 0.05\n",
      "Best accuracy: 0.9925709140027015 Regularization: 0.1\n",
      "Best accuracy: 0.9959477712742009 Regularization: 0.5\n",
      "Best accuracy: 0.9961728950923008 Regularization: 1\n",
      "Best accuracy: 0.9968482665466006 Regularization: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [02:18, 23.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.9970733903647006 Regularization: 5\n",
      "CAV accuracy for wood concept (17): 0.9971\n",
      "[0804_105617] 7/623 Calculating CAV for building\n",
      "Best accuracy: 0.9099504727600181 Regularization: 0.001\n",
      "Best accuracy: 0.9104007203962179 Regularization: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [02:39, 22.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for building concept (18): 0.9104\n",
      "[0804_105638] 8/623 Calculating CAV for person\n",
      "Best accuracy: 0.8354344889689329 Regularization: 0.001\n",
      "Best accuracy: 0.84984241332733 Regularization: 0.005\n",
      "Best accuracy: 0.85029266096353 Regularization: 0.01\n",
      "Best accuracy: 0.8520936515083296 Regularization: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [03:04, 23.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAV accuracy for person concept (19): 0.8521\n",
      "[0804_105704] 9/623 Calculating CAV for head\n"
     ]
    }
   ],
   "source": [
    "cavs = {}\n",
    "train_attributes = dense_attributes['train']\n",
    "val_attributes = dense_attributes['val']\n",
    "\n",
    "resume = False\n",
    "print(train_attributes.shape)\n",
    "\n",
    "cavs_save_dir = os.path.join('saved', 'ADE20K', 'cav', 'weighted', datetime.now().strftime(r'%m%d_%H%M%S'))\n",
    "if not resume:\n",
    "    ensure_dir(cavs_save_dir)\n",
    "    # if os.path.exists(cavs_save_dir):\n",
    "    #     raise ValueError(\"Path {} already exists\".format(cavs_save_dir))\n",
    "else:\n",
    "    timestamp = '0727_111655'\n",
    "    cavs_save_dir = os.path.join(os.path.dirname(cavs_save_dir), timestamp)\n",
    "\n",
    "\n",
    "cavs_save_path = os.path.join(cavs_save_dir, 'cavs.pickle')\n",
    "\n",
    "if resume:\n",
    "    with open(cavs_save_path, 'rb') as file:\n",
    "        cavs = pickle.load(file)\n",
    "    saved_attr_idxs = list(cavs.keys())\n",
    "    last_saved_idx = max(saved_attr_idxs)\n",
    "else:\n",
    "    last_saved_idx = -1\n",
    "\n",
    "log_path = os.path.join(cavs_save_dir, 'log.txt')\n",
    "n_frequent_attributes = len(frequent_attribute_idxs)\n",
    "\n",
    "for idx, attribute_idx in tqdm(enumerate(frequent_attribute_idxs)):\n",
    "    if attribute_idx < last_saved_idx:\n",
    "        continue\n",
    "    informal_log(\"[{}] {}/{} Calculating CAV for {}\".format(\n",
    "        datetime.now().strftime(r'%m%d_%H%M%S'),\n",
    "        idx+1,\n",
    "        n_frequent_attributes,\n",
    "        attribute_label_dict[attribute_idx]), log_path)\n",
    "    scaler = None # Preprocessing.StandardScaler()\n",
    "    \n",
    "    logistic_regression_args = {\n",
    "        'solver': 'liblinear',\n",
    "        'penalty': 'l2',\n",
    "        'class_weight': 'balanced'\n",
    "    }\n",
    "    cav = hyperparam_search(\n",
    "        train_features=train_features,\n",
    "        train_labels=train_attributes[:, idx],\n",
    "        val_features=val_features,\n",
    "        val_labels=val_attributes[:, idx],\n",
    "        scaler=scaler,\n",
    "        log_path=log_path,\n",
    "        logistic_regression_args=logistic_regression_args)\n",
    "    cavs[attribute_idx] = cav\n",
    "    \n",
    "    accuracy = cav.score(val_features, val_attributes[:, idx])\n",
    "    informal_log(\"CAV accuracy for {} concept ({}): {:.4f}\".format(\n",
    "        attribute_label_dict[attribute_idx],\n",
    "        attribute_idx,\n",
    "        accuracy), log_path)\n",
    "    # Save periodically\n",
    "    if idx % 10 == 0:\n",
    "        pickle.dump(cavs, open(cavs_save_path, 'wb'))\n",
    "        \n",
    "pickle.dump(cavs, open(cavs_save_path, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Concept present vectors from CAVs for each image in train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining concept presence vectors for train split\n",
      "[0. 1. 0. ... 0. 0. 1.]\n",
      "[1. 0. 1. ... 1. 0. 1.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 1. 0. 0.]\n",
      "[1. 0. 0. ... 0. 1. 1.]\n",
      "[1. 0. 1. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(13326, 182) 13326\n",
      "Obtaining concept presence vectors for val split\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[1. 0. 1. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 1. 0.]\n",
      "[1. 1. 0. ... 0. 1. 0.]\n",
      "[1. 1. 0. ... 0. 1. 0.]\n",
      "[0. 1. 0. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 1. 0. ... 1. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 1. 1. ... 0. 0. 1.]\n",
      "[0. 1. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 1. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 1. 0. ... 0. 1. 1.]\n",
      "[0. 1. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(4442, 182) 4442\n",
      "Obtaining concept presence vectors for test split\n",
      "[1. 1. 0. ... 1. 0. 1.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[1. 1. 0. ... 1. 0. 1.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 1. 0. 1.]\n",
      "[0. 1. 0. ... 1. 0. 1.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 1.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 1. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 1.]\n",
      "[0. 1. 0. ... 1. 0. 1.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[1. 1. 0. ... 1. 0. 0.]\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 1.]\n",
      "[0. 1. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 1. ... 0. 0. 1.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 1. 1. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 1. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(4442, 182) 4442\n",
      "Saved concept present vectors from CAVs to saved/ADE20K/cav/weighted/0517_151725/dense_cav_attributes.pth\n"
     ]
    }
   ],
   "source": [
    "# For images in train/val/test splits, predict the presence/absence of each concept from features using CAVs\n",
    "# Save as one-hot encoded concept-presence vectors\n",
    "# cavs_path = 'saved/ADE20K/cav/weighted/0517_151725/cavs.pickle'\n",
    "cavs_path = 'saved/ADE20K/cav/weighted/all_cavs.pickle'\n",
    "frequency_threshold = 1200\n",
    "n_concepts = 27\n",
    "frequent_attribute_idxs_path = 'data/ade20k/filter_attr_{}/frequent_attribute_idxs_n-{}.pth'.format(\n",
    "    frequency_threshold, n_concepts)\n",
    "\n",
    "use_dense = True\n",
    "\n",
    "cavs_save_dir = os.path.dirname(cavs_path)\n",
    "if 'scaled' in cavs_path:\n",
    "    scale = True\n",
    "else:\n",
    "    scale = False\n",
    "cavs = pickle.load(open(cavs_path, 'rb'))\n",
    "splits = ['train', 'val', 'test']\n",
    "frequent_attribute_idxs = torch.load(frequent_attribute_idxs_path)\n",
    "assert len(frequent_attribute_idxs) == len(cavs)\n",
    "\n",
    "if scale:\n",
    "    import sklearn.preprocessing as Preprocessing\n",
    "    scaler = Preprocessing.StandardScaler()\n",
    "    scaler.fit(features['train'])\n",
    "    scaled_features = {\n",
    "        'train': scaler.transform(features['train']),\n",
    "        'val': scaler.transform(features['val']),\n",
    "        'test': scaler.transform(features['test'])\n",
    "    }\n",
    "concept_vectors_dict = {}\n",
    "concept_vectors_save_path = os.path.join(cavs_save_dir, '{}_cav_attributes.pth'.format('dense' if use_dense else 'sparse'))\n",
    "if os.path.exists(concept_vectors_save_path):\n",
    "    print(\"Concept presence vectors already exist at {}\".format(concept_vectors_save_path))\n",
    "else:\n",
    "    for split in splits:\n",
    "        if scale:\n",
    "            split_features = scaled_features[split]\n",
    "        else:\n",
    "            split_features = features[split]\n",
    "        n_samples = len(split_features)\n",
    "        print(\"Obtaining concept presence vectors for {} split\".format(split))\n",
    "\n",
    "        concept_presence_vectors = []\n",
    "        \n",
    "        # If use_dense, save dense concept_presence_vectors\n",
    "        if use_dense:\n",
    "            for attr_idx in frequent_attribute_idxs:\n",
    "                cav = cavs[attr_idx]\n",
    "                concept_present = cav.predict(split_features)\n",
    "                print(concept_present)\n",
    "                assert len(concept_present) == n_samples\n",
    "                concept_presence_vectors.append(concept_present)\n",
    "                \n",
    "        else:\n",
    "            for attr_idx in tqdm(range(n_attributes)):\n",
    "                if attr_idx in cavs:\n",
    "                    cav = cavs[attr_idx]\n",
    "                    concept_present = cav.predict(split_features)\n",
    "                    assert len(concept_present) == n_samples\n",
    "                    concept_presence_vectors.append(concept_present)\n",
    "                    \n",
    "                else:\n",
    "                    concept_presence_vectors.append(np.zeros(n_samples))\n",
    "        concept_presence_vectors = np.stack(concept_presence_vectors, axis=1)\n",
    "        print(concept_presence_vectors.shape, n_samples)\n",
    "        # Concept vectors only for frequent vectors; Turn it into one hot vector\n",
    "\n",
    "        concept_vectors_dict[split] = concept_presence_vectors\n",
    "    \n",
    "    torch.save(concept_vectors_dict, concept_vectors_save_path)\n",
    "    print(\"Saved concept present vectors from CAVs to {}\".format(concept_vectors_save_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn Explainer from CAVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load attributes from CAVs\n",
    "cav_attributes_path = os.path.join(cavs_save_dir, 'cav_attributes.pth')\n",
    "cav_attributes = torch.load(cav_attributes_path)\n",
    "\n",
    "# Load model's predictions\n",
    "prediction_path = os.path.join(\n",
    "    'saved', \n",
    "    'PlacesCategoryClassification',\n",
    "    '0510_102912',\n",
    "    'ADE20K_predictions', \n",
    "    'saga', \n",
    "    '{}_outputs_predictions.pth')\n",
    "splits = ['train', 'val', 'test']\n",
    "predictions = {}\n",
    "for split in splits:\n",
    "    predictions[split] = torch.load(prediction_path.format(split))['predictions']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainer LogisticRegression(C=0.1, max_iter=200, multi_class='multinomial', penalty='l1',\n",
      "                   solver='saga') accuracy: 0.6960828455650608\n"
     ]
    }
   ],
   "source": [
    "# Train explainer\n",
    "logistic_regression_args = {\n",
    "    'solver': 'saga',\n",
    "    'penalty': 'l1'\n",
    "}\n",
    "\n",
    "max_iter = 200\n",
    "# explainer = hyperparam_search(\n",
    "#     train_features=cav_attributes['train'],\n",
    "#     train_labels=predictions['train'], \n",
    "#     val_features=cav_attributes['val'], \n",
    "#     val_labels=predictions['val'], \n",
    "#     scaler=None,\n",
    "#     Cs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5],\n",
    "#     log_path=None,\n",
    "#     logistic_regression_args=logistic_regression_args)\n",
    "explainer = LogisticRegression(\n",
    "    solver='saga',\n",
    "    penalty='l1',\n",
    "    C=0.1,\n",
    "    multi_class='multinomial',\n",
    "    max_iter=max_iter\n",
    ")\n",
    "explainer.fit(cav_attributes['train'], predictions['train'])\n",
    "accuracy = explainer.score(cav_attributes['val'], predictions['val'])\n",
    "print(\"Explainer {} accuracy: {}\".format(explainer, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved explainer trained on CAVs to saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/weighted_cav_explainer/saga_explainer_l1_0.1.pickle\n",
      "0.6960828455650608\n",
      "Saved outputs from validation set to saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/weighted_cav_explainer/saga_explainer_l1_0.1_validation.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4442it [00:00, 781276.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3092 congruent paths to saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/weighted_cav_explainer/congruent_paths.txt and 1350 incongruent paths to saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/weighted_cav_explainer/incongruent_paths.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "c = 0.1\n",
    "solver = 'saga'\n",
    "penalty = 'l1'\n",
    "save_dir = os.path.dirname(prediction_path)\n",
    "\n",
    "# Save CAV explainer\n",
    "cav_explainer_dir = os.path.join(save_dir, 'weighted_cav_explainer')\n",
    "ensure_dir(cav_explainer_dir)\n",
    "cav_explainer_save_path = os.path.join(cav_explainer_dir, '{}_explainer_{}_{}.pickle'.format(solver, penalty, c))\n",
    "if not os.path.exists(cav_explainer_save_path):\n",
    "    pickle.dump(explainer, open(cav_explainer_save_path, 'wb'))\n",
    "    print(\"Saved explainer trained on CAVs to {}\".format(cav_explainer_save_path))\n",
    "else:\n",
    "    print(\"Explainer already exists at '{}'\".format(cav_explainer_save_path))\n",
    "\n",
    "# Save CAV explainer predictions\n",
    "accuracy = explainer.score(cav_attributes['val'], predictions['val'])\n",
    "print(accuracy)\n",
    "\n",
    "explainer_outputs = explainer.decision_function(cav_attributes['val'])\n",
    "explainer_probabilities = explainer.predict_proba(cav_attributes['val'])\n",
    "explainer_predictions = explainer.predict(cav_attributes['val'])\n",
    "\n",
    "validation_output = {\n",
    "    'outputs': explainer_outputs,\n",
    "    'probabilities': explainer_probabilities,\n",
    "    'predictions': explainer_predictions\n",
    "}\n",
    "validation_output_path = os.path.join(cav_explainer_dir, '{}_explainer_{}_{}_validation.pth'.format(solver, penalty, c))\n",
    "if not os.path.exists(validation_output_path):\n",
    "    torch.save(validation_output, validation_output_path)\n",
    "    print(\"Saved outputs from validation set to {}\".format(validation_output_path))\n",
    "else:\n",
    "    print(\"Validation set outputs already saved to {}\".format(validation_output_path))\n",
    "\n",
    "# Save congruent/incongruent paths\n",
    "congruency_paths = partition_paths_by_congruency(\n",
    "    explainer_predictions=explainer_predictions,\n",
    "    model_predictions=predictions['val'],\n",
    "    paths=paths['val']\n",
    ")\n",
    "congruent_paths = congruency_paths['congruent']\n",
    "incongruent_paths = congruency_paths['incongruent']\n",
    "congruent_paths_path = os.path.join(cav_explainer_dir, 'congruent_paths.txt')\n",
    "incongruent_paths_path = os.path.join(cav_explainer_dir, 'incongruent_paths.txt')\n",
    "if not os.path.exists(congruent_paths_path) or not os.path.exists(incongruent_paths_path):\n",
    "    write_lists(congruent_paths, congruent_paths_path)\n",
    "    write_lists(incongruent_paths, incongruent_paths_path)\n",
    "    print(\"Saved {} congruent paths to {} and {} incongruent paths to {}\".format(\n",
    "        len(congruent_paths),\n",
    "        congruent_paths_path,\n",
    "        len(incongruent_paths),\n",
    "        incongruent_paths_path\n",
    "    ))\n",
    "else:\n",
    "    print(\"Congruent paths already saved to {} and incongruent paths already saved to {}\".format(\n",
    "        congruent_paths_path, incongruent_paths_path\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-correlation",
   "language": "python",
   "name": "model-correlation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c579736b944d6d0768ad80dbc3e033126adf2082032258477813cfb04cf1061b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
