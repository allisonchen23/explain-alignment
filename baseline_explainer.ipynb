{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a43608d-fba8-4ebf-80eb-9923937e810a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "import pickle\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "from utils.places365_pred_utils import get_class_category_dict, get_category_class_dict\n",
    "from utils.utils import ensure_dir, write_lists\n",
    "from utils.attribute_utils import get_one_hot_attributes, get_frequent_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d4eff-6e6a-40cd-8cc9-10d6fad75ff5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load attributes and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd4d23fd-1866-4364-ab5d-1ba1641cb51d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data keys: dict_keys(['train', 'val', 'test', 'labels'])\n",
      "(13326,) (4442,) (4442,)\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('data', 'ade20k', 'full_ade20k_imagelabels.pth')\n",
    "save_dir = os.path.join('saved', 'ADE20K', '0501_105640')\n",
    "predictions_path = os.path.join(save_dir, '{}_logits_predictions.pth')\n",
    "\n",
    "data = torch.load(data_path)\n",
    "print(\"Data keys: {}\".format(data.keys()))\n",
    "train = torch.load(predictions_path.format('train'))\n",
    "train_paths = train['paths']\n",
    "train_predictions = train['predictions']\n",
    "\n",
    "val = torch.load(predictions_path.format('val'))\n",
    "val_paths = val['paths']\n",
    "val_logits = val['logits']\n",
    "val_predictions = val['predictions']\n",
    "\n",
    "test = torch.load(predictions_path.format('test'))\n",
    "test_paths = test['paths']\n",
    "test_predictions = test['predictions']\n",
    "\n",
    "print(train_predictions.shape, val_predictions.shape, test_predictions.shape)\n",
    "predictions = {\n",
    "    'train': train_predictions,\n",
    "    'val': val_predictions,\n",
    "    'test': test_predictions\n",
    "}\n",
    "paths = {\n",
    "    'train': train_paths,\n",
    "    'val': val_paths,\n",
    "    'test': test_paths\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e398647-6568-4d1d-b460-ea79e407e567",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest attribute index: 12 Highest attribute index: 1197\n",
      "Processing attributes for train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13326/13326 [00:00<00:00, 101818.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing attributes for val split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4442/4442 [00:00<00:00, 118150.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing attributes for test split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4442/4442 [00:00<00:00, 135174.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# def get_attributes_set(data, paths):\n",
    "#     splits = ['train', 'val', 'test']\n",
    "#     seen_attributes = set()\n",
    "#     for split in splits:\n",
    "#         split_paths = paths[split]\n",
    "#         for path in split_paths:\n",
    "#             # Obtain attributes\n",
    "#             cur_attributes = data['labels'][path]\n",
    "#             for attr in cur_attributes:\n",
    "#                 seen_attributes.add(attr)\n",
    "#     return seen_attributes\n",
    "\n",
    "# def get_one_hot_attributes(data, paths, n_attr):\n",
    "#     splits = ['train', 'val', 'test']\n",
    "#     attributes = {\n",
    "#         'train': [],\n",
    "#         'val': [], \n",
    "#         'test': []\n",
    "#     }\n",
    "    \n",
    "#     for split in splits:\n",
    "#         split_paths = paths[split]\n",
    "#         print(\"Processing attributes for {} split\".format(split))\n",
    "#         for path in tqdm(split_paths):\n",
    "#             # Obtain attributes and covnvert to one hot\n",
    "#             cur_attributes = data['labels'][path]\n",
    "#             one_hot_attributes = np.zeros(n_attr)\n",
    "#             one_hot_attributes[cur_attributes] = 1\n",
    "#             attributes[split].append(one_hot_attributes)\n",
    "#         attributes[split] = np.stack(attributes[split], axis=0)\n",
    "        \n",
    "#     return attributes\n",
    "\n",
    "# attr_set = get_attributes_set(data, paths)\n",
    "# n_attr = len(attr_set)\n",
    "\n",
    "# print(\"Lowest attribute index: {} Highest attribute index: {}\".format(min(attr_set), max(attr_set)))\n",
    "# N_ATTR = 1200  # including the attributes\n",
    "\n",
    "# attributes = get_one_hot_attributes(data, paths, N_ATTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2b8bfa7-eef9-4c1c-b60c-6fa593df2d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182 concepts that occur > 150 times in training\n",
      "623 concepts that occur at all in training\n"
     ]
    }
   ],
   "source": [
    "# train_attributes = attributes['train']\n",
    "# counts = np.sum(train_attributes, axis=0)\n",
    "# print(\"{} concepts that occur > 150 times in training\".format(len(np.where(counts > 150)[0])))\n",
    "# print(\"{} concepts that occur at all in training\".format(len(np.nonzero(counts)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad38fa7-4284-4e04-b04b-25c666307d5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Obtain attributes that are frequently occuring in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d281133f-ffb5-4a6f-a87d-44113f34f040",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 examples have no more attributes\n",
      "8308/13326 examples affected\n",
      "3 examples have no more attributes\n",
      "2736/4442 examples affected\n",
      "3 examples have no more attributes\n",
      "2804/4442 examples affected\n"
     ]
    }
   ],
   "source": [
    "# FREQUENCY_THRESH = 150\n",
    "# n_samples = attributes['train'].shape[0]\n",
    "# train_counts = np.sum(attributes['train'], axis=0)\n",
    "\n",
    "# def obtain_frequent_attributes(cur_attributes, train_counts):\n",
    "\n",
    "#     # Obtain one-hot encoding of attributes that exceed frequency threshold\n",
    "#     frequent_attributes_one_hot = np.where(train_counts > FREQUENCY_THRESH, 1, 0)\n",
    "#     # Mask out infrequent attributes\n",
    "#     frequent_attributes = np.where(frequent_attributes_one_hot == 1, cur_attributes, 0)\n",
    "\n",
    "#     # Sanity checks\n",
    "#     discarded_attributes_idxs = np.nonzero(np.where(train_counts < FREQUENCY_THRESH, 1, 0))[0]\n",
    "#     kept_attributes_idxs = np.nonzero(train_counts > FREQUENCY_THRESH)[0]\n",
    "#     assert (kept_attributes_idxs == np.nonzero(frequent_attributes_one_hot)[0]).all()\n",
    "\n",
    "#     zeroed_ctr = 0\n",
    "#     ctr = 0\n",
    "#     for idx, (orig, new) in enumerate(zip(cur_attributes, frequent_attributes)):\n",
    "#         # print(orig\n",
    "#         if not (orig == new).all():\n",
    "#             orig_idxs = np.nonzero(orig)[0]\n",
    "#             new_idxs = np.nonzero(new)[0]\n",
    "#             # Assert new idxs ONLY contains the kept attributes and none of discarded\n",
    "#             assert len(np.intersect1d(new_idxs, discarded_attributes_idxs)) == 0\n",
    "#             assert len(np.intersect1d(new_idxs, kept_attributes_idxs)) == len(new_idxs)\n",
    "#             # Assert overlap with original indices is equal to new indices\n",
    "#             assert (np.intersect1d(orig_idxs, new_idxs) == new_idxs).all()\n",
    "#             if len(new_idxs) == 0:\n",
    "#                 zeroed_ctr += 1\n",
    "#             ctr += 1\n",
    "#     print(\"{} examples have no more attributes\".format(zeroed_ctr))\n",
    "#     print(\"{}/{} examples affected\".format(ctr, len(cur_attributes)))\n",
    "    \n",
    "#     return frequent_attributes\n",
    "\n",
    "# frequent_attributes = {}\n",
    "# for split in ['train', 'val', 'test']:\n",
    "#     cur_frequent_attributes = obtain_frequent_attributes(\n",
    "#         cur_attributes=attributes[split],\n",
    "#         train_counts = train_counts)\n",
    "#     frequent_attributes[split] = cur_frequent_attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_attributes = 1200\n",
    "frequency_threshold = 150\n",
    "attributes = get_one_hot_attributes(\n",
    "    data=data,\n",
    "    paths=paths,\n",
    "    n_attr=n_attributes\n",
    ")\n",
    "\n",
    "frequent_attributes, frequent_attributes_one_hot = get_frequent_attributes(\n",
    "    attributes=attributes,\n",
    "    frequency_threshold=frequency_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1693f7e0-3e8d-4834-a0f3-880345f8fc14",
   "metadata": {},
   "source": [
    "#### Obtain scene category predictions from 365-way scene prediction and save to `save_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fd38717-dc47-467b-b20f-fc573c656b77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved/ADE20K/0501_105640/train_scene_category_predictions.pth already exists.\n",
      "saved/ADE20K/0501_105640/val_scene_category_predictions.pth already exists.\n",
      "saved/ADE20K/0501_105640/test_scene_category_predictions.pth already exists.\n"
     ]
    }
   ],
   "source": [
    "scene_category_dict = get_class_category_dict()\n",
    "\n",
    "def get_scene_category_predictions(scene_predictions, scene_category_dict):\n",
    "    category_predictions = []\n",
    "    for scene_prediction in scene_predictions:\n",
    "        category_predictions.append(scene_category_dict[scene_prediction])\n",
    "    \n",
    "    category_predictions = np.array(category_predictions)\n",
    "    return category_predictions\n",
    "\n",
    "# Save category predictions in save_dir\n",
    "category_predictions = {}\n",
    "for split, split_predictions in predictions.items():\n",
    "    split_category_predictions = get_scene_category_predictions(\n",
    "        scene_predictions=split_predictions,\n",
    "        scene_category_dict=scene_category_dict)\n",
    "    \n",
    "    save_path = os.path.join(save_dir, '{}_scene_category_predictions.pth'.format(split))\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"{} already exists.\".format(save_path))\n",
    "    else:\n",
    "        torch.save({'scene_category_predictions': split_category_predictions}, save_path)\n",
    "        print(\"Saved scene category predictions for {} to {}\".format(split, save_path))\n",
    "    category_predictions[split] = split_category_predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c10b26-9497-4867-8e31-d2f9eb88cf4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run hyperparameter search on logistic regression for linear model to predict classes from attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9624bc97-4510-471f-b8b9-ab62647e20c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hyperparam_search_l1(train_features, train_labels, val_features, val_labels, \n",
    "                      Cs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5]):\n",
    "    best_clf = None\n",
    "    best_acc = 0\n",
    "    \n",
    "    for c in Cs:\n",
    "        clf = LogisticRegression(solver='liblinear', C=c, penalty='l1')\n",
    "        clf.fit(train_features, train_labels)\n",
    "        score = clf.score(val_features, val_labels)\n",
    "        if score>best_acc:\n",
    "            best_acc = score\n",
    "            best_clf = clf\n",
    "            print(\"Best accuracy: {} Regularization: {}\".format(score, c))\n",
    "    \n",
    "    return best_clf\n",
    "\n",
    "def hyperparam_search(train_features,\n",
    "                                  train_labels, \n",
    "                                  val_features, \n",
    "                                  val_labels, \n",
    "                                  regularization,\n",
    "                                  solver,\n",
    "                                  scaler=None,\n",
    "                                  Cs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 3, 5]):\n",
    "    best_clf = None\n",
    "    best_acc = 0\n",
    "    \n",
    "    if scaler is not None:\n",
    "        scaler.fit(train_features)\n",
    "        print(\"Scaler parameters: {}\".format(scaler.get_params()))\n",
    "        train_features = scaler.transform(train_features)\n",
    "        val_features = scaler.transform(val_features)\n",
    "    for c in Cs:\n",
    "        clf = LogisticRegression(solver=solver, C=c, penalty=regularization)\n",
    "        clf.fit(train_features, train_labels)\n",
    "        score = clf.score(val_features, val_labels)\n",
    "        if score>best_acc:\n",
    "            best_acc = score\n",
    "            best_clf = clf\n",
    "            print(\"Best accuracy: {} Regularization: {}\".format(score, c))\n",
    "    \n",
    "    return best_clf\n",
    "\n",
    "def partition_paths_by_congruency(explainer_predictions,\n",
    "                                  model_predictions,\n",
    "                                  paths):\n",
    "    '''\n",
    "    Given list or arrays of explainer and model predictions, partition paths based on if predictions align\n",
    "\n",
    "    Arg(s):\n",
    "        explainer_predictions : N-length np.array\n",
    "            predictions output by the explainer model\n",
    "        model_predictions : N-length np.array\n",
    "            predictions output by the model\n",
    "        paths : N-length list\n",
    "            paths of images corresponding to each data point\n",
    "\n",
    "    Returns:\n",
    "        dictionary : dict[str] : list\n",
    "            key: 'congruent' or 'incongruent'\n",
    "            value: list of paths\n",
    "    '''\n",
    "    n_samples = len(paths)\n",
    "    assert len(explainer_predictions) == n_samples\n",
    "    assert len(model_predictions) == n_samples, \"Length of model predictions {} doesn't match n_samples {}\".format(\n",
    "        len(model_predictions), n_samples\n",
    "    )\n",
    "\n",
    "    incongruent_paths = []\n",
    "    congruent_paths = []\n",
    "\n",
    "    for explainer_prediction, model_prediction, path in tqdm(zip(\n",
    "        explainer_predictions, model_predictions, paths\n",
    "    )):\n",
    "        if explainer_prediction == model_prediction:\n",
    "            congruent_paths.append(path)\n",
    "        else:\n",
    "            incongruent_paths.append(path)\n",
    "    \n",
    "    return {\n",
    "        'congruent': congruent_paths,\n",
    "        'incongruent': incongruent_paths\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a2e66-03f4-451b-bd1d-3dc153accb70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Predicting all 365 classes with all part/object attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecdfe93b-ef53-4cc0-85e0-dd9fb7386abe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029941467807294012 0.001\n",
      "0.1154885186852769 0.005\n",
      "0.15038271049076993 0.01\n",
      "0.23660513282305268 0.05\n",
      "0.28140477262494373 0.1\n",
      "0.3685276902296263 0.5\n",
      "0.375056280954525 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam_search_l1(\n",
    "    train_features=attributes['train'],\n",
    "    train_labels=train_predictions,\n",
    "    val_features=attributes['val'],\n",
    "    val_labels=val_predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3222903-b336-4e1f-bf1d-4bc21d0876d0",
   "metadata": {},
   "source": [
    "#### Predicting all  365 classes with part/object attributes that appear in >=150 training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "594d82ff-043e-4634-9fec-1411815020cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.029941467807294012 Regularization: 0.001\n",
      "Best accuracy: 0.1154885186852769 Regularization: 0.005\n",
      "Best accuracy: 0.14678072940117065 Regularization: 0.01\n",
      "Best accuracy: 0.20711391265195858 Regularization: 0.05\n",
      "Best accuracy: 0.24245835209365152 Regularization: 0.1\n",
      "Best accuracy: 0.285006753714543 Regularization: 0.5\n",
      "Best accuracy: 0.2915353444394417 Regularization: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam_search_l1(\n",
    "    train_features=freq_train_attributes,\n",
    "    train_labels=train_predictions,\n",
    "    val_features=freq_val_attributes,\n",
    "    val_labels=val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cddda3-30ad-4b29-868a-cf9b15730beb",
   "metadata": {},
   "source": [
    "#### Predict only 16 scene categories with all part/object attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "40e57804-92da-40be-a274-bddb97e4ecfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.3275551553354345 Regularization: 0.001\n",
      "Best accuracy: 0.39756866276452046 Regularization: 0.005\n",
      "Best accuracy: 0.43876632147681227 Regularization: 0.01\n",
      "Best accuracy: 0.5538045925258892 Regularization: 0.05\n",
      "Best accuracy: 0.5839711841512832 Regularization: 0.1\n",
      "Best accuracy: 0.6226924808644755 Regularization: 0.5\n",
      "Best accuracy: 0.6292210715893741 Regularization: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam_search_l1(\n",
    "    train_features=attributes['train'],\n",
    "    train_labels=category_predictions['train'],\n",
    "    val_features=attributes['val'],\n",
    "    val_labels=category_predictions['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b07d46-28d2-43a7-afea-b33058fe1979",
   "metadata": {},
   "source": [
    "#### Predict 16 scene categories with top attributes (appear in 150 or more training examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b77a0-bbef-45ce-9282-a248bad35f97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial solver\n",
      "Best accuracy: 0.42976136875281407 Regularization: 0.005\n",
      "Best accuracy: 0.4558757316524088 Regularization: 0.01\n",
      "Best accuracy: 0.539846915803692 Regularization: 0.05\n",
      "Best accuracy: 0.5520036019810896 Regularization: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/ac-project/anaconda3/envs/model-correlation/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.5632597928860873 Regularization: 0.5\n"
     ]
    }
   ],
   "source": [
    "# hyperparam_search_l1(\n",
    "#     train_features=frequent_attributes['train'],\n",
    "#     train_labels=category_predictions['train'],\n",
    "#     val_features=frequent_attributes['val'],\n",
    "#     val_labels=category_predictions['val'])\n",
    "\n",
    "print(\"Multinomial solver\")\n",
    "hyperparam_search(\n",
    "    train_features=frequent_attributes['train'],\n",
    "    train_labels=category_predictions['train'],\n",
    "    val_features=frequent_attributes['val'],\n",
    "    val_labels=category_predictions['val'],\n",
    "    solver='saga',\n",
    "    regularization='l1',\n",
    "    Cs=[0.005, 0.01, 0.05, 0.1, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454e6603-c02a-45ae-960e-5691270f204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.5612336785231877 Regularization: 0.2\n",
      "Best accuracy: 0.5628095452498875 Regularization: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/fs/ac-project/anaconda3/envs/model-correlation/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.4, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.4, penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.4, penalty='l1', solver='saga')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam_search(\n",
    "    train_features=frequent_attributes['train'],\n",
    "    train_labels=category_predictions['train'],\n",
    "    val_features=frequent_attributes['val'],\n",
    "    val_labels=category_predictions['val'],\n",
    "    solver='saga',\n",
    "    regularization='l1',\n",
    "    Cs=[0.2, 0.3, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec682c7",
   "metadata": {},
   "source": [
    "### Create Explainer Baseline Using Multinomial and L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7995ca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression parameters: \n",
      "\tC: 0.5 \tSolver: saga \tPenalty: l1\n",
      "Fit logistic regression on training data\n"
     ]
    }
   ],
   "source": [
    "solver = 'saga'\n",
    "penalty = 'l1'\n",
    "c = 0.5\n",
    "max_iter = 200\n",
    "\n",
    "train_X = frequent_attributes['train']\n",
    "val_X = frequent_attributes['val']\n",
    "\n",
    "train_y = category_predictions['train']\n",
    "val_y = category_predictions['val']\n",
    "\n",
    "# Create logistic regression classifier and predict for validation set\n",
    "print(\"Logistic regression parameters: \\n\\tC: {} \\tSolver: {} \\tPenalty: {}\".format(\n",
    "    c, solver, penalty))\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    solver=solver, \n",
    "    C=c, \n",
    "    penalty=penalty,\n",
    "    max_iter=max_iter)\n",
    "\n",
    "logreg.fit(train_X, train_y)\n",
    "\n",
    "print(\"Fit logistic regression on training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5d5a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5634849167041873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4442it [00:00, 1268111.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2503 congruent an 1939 incongruent samples\n",
      "Saved explainer to saved/ADE20K/0501_105640/baseline_explainer/explainer_saga_l1_0.5.pickle\n",
      "Saved congruent and incongruent paths to saved/ADE20K/0501_105640/baseline_explainer/congruent_paths.txt and saved/ADE20K/0501_105640/baseline_explainer/incongruent_paths.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print(\"Accuracy: {}\".format(logreg.score(val_X, val_y)))\n",
    "\n",
    "# Save congruent/incongruent paths\n",
    "ade20k_imagelabels = torch.load('/n/fs/ac-alignment/explain-alignment/data/ade20k/full_ade20k_imagelabels.pth')\n",
    "paths = ade20k_imagelabels['val']\n",
    "explainer_predictions = logreg.predict(val_X)\n",
    "model_predictions = val_y\n",
    "\n",
    "path_congruency = partition_paths_by_congruency(\n",
    "    explainer_predictions=explainer_predictions,\n",
    "    model_predictions=model_predictions,\n",
    "    paths=paths\n",
    ")\n",
    "\n",
    "congruent_paths = path_congruency['congruent']\n",
    "incongruent_paths = path_congruency['incongruent']\n",
    "print(\"{} congruent an {} incongruent samples\".format(len(congruent_paths), len(incongruent_paths)))\n",
    "\n",
    "explainer_save_path = os.path.join(save_dir, 'baseline_explainer', \n",
    "'explainer_{}_{}_{}.pickle'.format(solver, penalty, c))\n",
    "if os.path.exists(explainer_save_path):\n",
    "    print(\"Path '{}' already exists\".format(explainer_save_path))\n",
    "else:\n",
    "    pickle.dump(logreg, open(explainer_save_path, 'wb'))\n",
    "    congruent_paths_path = os.path.join(os.path.dirname(explainer_save_path), 'congruent_paths.txt')\n",
    "    incongruent_paths_path = os.path.join(os.path.dirname(explainer_save_path), 'incongruent_paths.txt')\n",
    "\n",
    "    write_lists(congruent_paths, congruent_paths_path)\n",
    "    write_lists(incongruent_paths, incongruent_paths_path)\n",
    "    \n",
    "    print(\"Saved explainer to {}\".format(explainer_save_path))\n",
    "    print(\"Saved congruent and incongruent paths to {} and {}\".format(congruent_paths_path, incongruent_paths_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a783274",
   "metadata": {},
   "source": [
    "### Load 16-way scene category predictions from linear models trained to predict scene categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc88d79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter search for linear_layer\n",
      "Best accuracy: 0.5666366501575867 Regularization: 0.05\n",
      "Best accuracy: 0.590274651058082 Regularization: 0.1\n",
      "Best accuracy: 0.6046825754164791 Regularization: 0.5\n",
      "Best accuracy: 0.6055830706888788 Regularization: 1\n",
      "Best accuracy: 0.6060333183250788 Regularization: 3\n",
      "Hyperparameter search for liblinear\n",
      "Best accuracy: 0.5914002701485818 Regularization: 0.05\n",
      "Best accuracy: 0.6141377757766772 Regularization: 0.1\n",
      "Best accuracy: 0.6296713192255741 Regularization: 0.5\n",
      "Best accuracy: 0.6303466906798739 Regularization: 1\n",
      "Hyperparameter search for saga\n",
      "Best accuracy: 0.5945520036019811 Regularization: 0.05\n",
      "Best accuracy: 0.6114362899594777 Regularization: 0.1\n",
      "Best accuracy: 0.6269698334083746 Regularization: 0.5\n",
      "Best accuracy: 0.6278703286807744 Regularization: 1\n",
      "Best accuracy: 0.6312471859522738 Regularization: 3\n",
      "Best accuracy: 0.6314723097703737 Regularization: 5\n"
     ]
    }
   ],
   "source": [
    "linear_ids = ['linear_layer', 'liblinear', 'saga']\n",
    "restore_path_template = os.path.join('saved', 'PlacesCategoryClassification', '0510_102912',\n",
    "    'ADE20K_predictions', '{}', '{}_outputs_predictions.pth')\n",
    "\n",
    "# splits =['train', 'val']\n",
    "for id in linear_ids:\n",
    "    cur_train_predictions = torch.load(restore_path_template.format(id, 'train'))['predictions']\n",
    "    cur_val_predictions = torch.load(restore_path_template.format(id, 'val'))['predictions']\n",
    "\n",
    "    print(\"Hyperparameter search for {}\".format(id))\n",
    "    explainer = hyperparam_search_l1(\n",
    "        train_features=frequent_attributes['train'], \n",
    "        train_labels=cur_train_predictions, \n",
    "        val_features=frequent_attributes['val'], \n",
    "        val_labels=cur_val_predictions, \n",
    "        Cs = [0.05, 0.1, 0.5, 1, 3, 5, 7])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75db8960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for linear_layer's explainer: 0.6060333183250788\n",
      "Explainer already saved to saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/linear_layer/baseline_explainer/linear_layer_explainer_l1_3.pickle\n",
      "Accuracy for liblinear's explainer: 0.630121566861774\n",
      "Explainer already saved to saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/liblinear/baseline_explainer/liblinear_explainer_l1_1.pickle\n",
      "Accuracy for saga's explainer: 0.6314723097703737\n",
      "Explainer already saved to saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/baseline_explainer/saga_explainer_l1_5.pickle\n"
     ]
    }
   ],
   "source": [
    "# Create explainer methods for each 16-way scene classifier\n",
    "linear_ids = ['linear_layer', 'liblinear', 'saga']\n",
    "Cs = [3, 1, 5] # best regularization values found based on h-param search\n",
    "restore_path_template = os.path.join('saved', 'PlacesCategoryClassification', '0510_102912',\n",
    "    'ADE20K_predictions', '{}', '{}_outputs_predictions.pth')\n",
    "\n",
    "solver = 'liblinear'\n",
    "penalty = 'l1'\n",
    "\n",
    "cur_train_features = frequent_attributes['train']\n",
    "cur_val_features = frequent_attributes['val']\n",
    "\n",
    "explainers = {}\n",
    "for c, linear_id in zip(Cs, linear_ids):\n",
    "    cur_train_predictions = torch.load(restore_path_template.format(linear_id, 'train'))['predictions']\n",
    "    cur_val_predictions = torch.load(restore_path_template.format(linear_id, 'val'))['predictions']\n",
    "\n",
    "    explainer = LogisticRegression(\n",
    "        solver=solver,\n",
    "        penalty=penalty,\n",
    "        C=c\n",
    "    )\n",
    "\n",
    "    explainer.fit(cur_train_features, cur_train_predictions)\n",
    "    accuracy = explainer.score(cur_val_features, cur_val_predictions)\n",
    "    print(\"Accuracy for {}'s explainer: {}\".format(linear_id, accuracy))\n",
    "\n",
    "    # Save explainer, decision_function, probabilities, and predictions\n",
    "    explainer_name = '{}_explainer_l1_{}'.format(linear_id, c)\n",
    "    save_dir = os.path.join(os.path.dirname(restore_path_template).format(linear_id), 'baseline_explainer')\n",
    "    ensure_dir(save_dir)\n",
    "    explainer_save_path = os.path.join(save_dir, '{}.pickle'.format(explainer_name))\n",
    "    \n",
    "    save_data = {\n",
    "        'outputs': explainer.decision_function(cur_val_features),\n",
    "        'probabilities': explainer.predict_proba(cur_val_features),\n",
    "        'predictions': explainer.predict(cur_val_features)\n",
    "    }\n",
    "    data_save_path = os.path.join(save_dir, '{}_validation.pth'.format(explainer_name))\n",
    "\n",
    "    if os.path.exists(explainer_save_path):\n",
    "        print(\"Explainer already saved to {}\".format(explainer_save_path))\n",
    "    else:\n",
    "        pickle.dump(explainer, open(explainer_save_path, 'wb'))\n",
    "        torch.save(save_data, data_save_path)\n",
    "        print(\"Saved explainer and outputs on validation set to {}\".format(os.path.dirname(explainer_save_path)))\n",
    "\n",
    "    explainers[linear_id] = {\n",
    "        'explainer_model': explainer,\n",
    "        'validation': save_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ceb4c",
   "metadata": {},
   "source": [
    "#### Determine congruency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be51f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def partition_paths_by_congruency(explainer_predictions,\n",
    "#                                   model_predictions,\n",
    "#                                   paths):\n",
    "#     n_samples = len(paths)\n",
    "#     assert len(explainer_predictions) == n_samples\n",
    "#     assert len(model_predictions) == n_samples, \"Length of model predictions {} doesn't match n_samples {}\".format(\n",
    "#         len(model_predictions), n_samples\n",
    "#     )\n",
    "\n",
    "#     incongruent_paths = []\n",
    "#     congruent_paths = []\n",
    "\n",
    "#     for explainer_prediction, model_prediction, path in tqdm(zip(\n",
    "#         explainer_predictions, model_predictions, paths\n",
    "#     )):\n",
    "#         if explainer_prediction == model_prediction:\n",
    "#             congruent_paths.append(path)\n",
    "#         else:\n",
    "#             incongruent_paths.append(path)\n",
    "    \n",
    "#     return {\n",
    "#         'congruent': congruent_paths,\n",
    "#         'incongruent': incongruent_paths\n",
    "#     }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7409c8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cav_explainer accuracy: 0.6517334533993696\n",
      "base_explainer accuracy: 0.6314723097703737\n",
      "0.5335434488968933\n"
     ]
    }
   ],
   "source": [
    "cav_explainer = torch.load('saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/scaled_cav_explainer/saga_explainer_l1_0.5_validation.pth')\n",
    "baseline_explainer = torch.load('saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/baseline_explainer/saga_explainer_l1_5_validation.pth')\n",
    "model_outputs = torch.load('saved/PlacesCategoryClassification/0510_102912/ADE20K_predictions/saga/val_outputs_predictions.pth')\n",
    "\n",
    "cav_predictions = cav_explainer['predictions']\n",
    "baseline_predictions = baseline_explainer['predictions']\n",
    "model_predictions = model_outputs['predictions']\n",
    "print(\"cav_explainer accuracy: {}\".format(np.count_nonzero(cav_predictions == model_predictions) / len(cav_predictions)))\n",
    "\n",
    "print(\"base_explainer accuracy: {}\".format(np.count_nonzero(baseline_predictions == model_predictions) / len(baseline_predictions)))\n",
    "print(np.count_nonzero(np.logical_and(cav_predictions == model_predictions, cav_predictions == baseline_predictions)) / len(cav_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83cf4e4-5a83-4e11-9f5f-74f3e70cc8ff",
   "metadata": {},
   "source": [
    "### Obtain predictions of linear classifier on validation set (16 way classifier with only frequent attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4263c509-e66e-4c0f-a418-9e673a190e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression parameters: \n",
      "\tC: 0.2 \tSolver: saga \tPenalty: l1\n",
      "Fit logistic regression on training data\n",
      "Accuracy: 0.5612336785231877\n"
     ]
    }
   ],
   "source": [
    "# Define variables\n",
    "c = 0.2\n",
    "solver = 'saga'\n",
    "penalty = 'l1'\n",
    "\n",
    "train_X = frequent_attributes['train']\n",
    "val_X = frequent_attributes['val']\n",
    "\n",
    "train_y = category_predictions['train']\n",
    "val_y = category_predictions['val']\n",
    "\n",
    "# Create logistic regression classifier and predict for validation set\n",
    "logreg = LogisticRegression(\n",
    "    solver=solver, \n",
    "    C=c, \n",
    "    penalty=penalty)\n",
    "print(\"Logistic regression parameters: \\n\\tC: {} \\tSolver: {} \\tPenalty: {}\".format(\n",
    "    c, solver, penalty))\n",
    "logreg.fit(train_X, train_y)\n",
    "\n",
    "print(\"Fit logistic regression on training data\")\n",
    "\n",
    "logreg_category_predictions = logreg.predict(val_X)\n",
    "\n",
    "n_samples = len(val_y)\n",
    "assert len(logreg_category_predictions) == n_samples\n",
    "assert len(val_paths) == n_samples\n",
    "\n",
    "print(\"Accuracy: {}\".format(1 - np.count_nonzero(logreg_category_predictions != val_y) / n_samples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dd0ea79-5ceb-4a40-a29f-0c6b17c34669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incongruent_paths = []\n",
    "# congruent_paths = []\n",
    "# baseline_save_dir = os.path.join(save_dir, 'baseline_explainer')\n",
    "# ensure_dir(baseline_save_dir)\n",
    "# param_string = '16_class_freq_attr'\n",
    "# incongruent_save_path = os.path.join(baseline_save_dir, 'incongruent_paths_{}_{}_{}_{}.txt'.format(param_string, solver, penalty, c))\n",
    "# congruent_save_path = os.path.join(baseline_save_dir, 'congruent_paths_{}_{}_{}_{}.txt'.format(param_string, solver, penalty, c))\n",
    "\n",
    "# for logreg_pred, model_pred, image_path in tqdm(zip(logreg_category_predictions, val_y, val_paths)):\n",
    "#     if logreg_pred != model_pred:\n",
    "#         incongruent_paths.append(image_path)\n",
    "#     else:\n",
    "#         congruent_paths.append(image_path)\n",
    "\n",
    "# for save_path, paths_list in zip([incongruent_save_path, congruent_save_path], [incongruent_paths, congruent_paths]):\n",
    "#     if os.path.exists(save_path):\n",
    "#         print(\"{} already exists\".format(save_path))\n",
    "#     else:\n",
    "#         write_lists(save_path, paths_list)\n",
    "#         print(\"Saved {} paths to {}\".format(len(paths_list), save_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b881a-1432-4a05-b131-f532a64c69ec",
   "metadata": {},
   "source": [
    "#### Obtain the pre-normalized output and normalized probabilities from the logistic regressor on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccf6e3f3-9062-46f9-8adf-733c69c372be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -6.21541755  -9.57385951 -11.01464339  -8.19257013  -8.35659567\n",
      "  -2.3154842   -5.336674    -7.39402475  -2.75187164  -5.73843319\n",
      "  -3.09532866   0.10664419  -1.61541948  -4.4823798   -1.89333494\n",
      "  -2.06268626] [1.73180361e-03 6.03646460e-05 1.42916118e-05 2.40203346e-04\n",
      " 2.03873998e-04 7.80187178e-02 4.15828632e-03 5.33624900e-04\n",
      " 5.20836190e-02 2.78689906e-03 3.75992050e-02 4.57296199e-01\n",
      " 1.44002514e-01 9.70804106e-03 1.13634363e-01 9.79279926e-02] 1.0\n"
     ]
    }
   ],
   "source": [
    "outputs = logreg.decision_function(val_X)\n",
    "output_probabilities = logreg.predict_proba(val_X)\n",
    "print(outputs[0], output_probabilities[0], np.sum(output_probabilities[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dedfb814-76e3-487c-b56c-7aed6ec75f77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4442,)\n",
      "1103\n",
      "4437\n",
      "4434\n",
      "[ 8 15 15  2  2] [209 112 125  52  45]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-correlation",
   "language": "python",
   "name": "model-correlation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
